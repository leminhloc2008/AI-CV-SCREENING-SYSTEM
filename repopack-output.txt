================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2025-02-14T15:04:59.309Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
.gitignore
app/data/companies.json
app/data/technical_terms.json
app/data/universities.json
app/main.py
app/models/resume.py
app/models/scoring_rules.py
app/modules/document_extraction/extractor.py
app/modules/document_extraction/gpa_parser.py
app/modules/document_extraction/ocr.py
app/modules/embedding/embedder.py
app/modules/embedding/similarity.py
app/modules/scoring/awards.py
app/modules/scoring/certifications.py
app/modules/scoring/education.py
app/modules/scoring/experience.py
app/modules/scoring/projects.py
app/modules/scoring/scorer.py
app/modules/summarization/evaluator.py
app/modules/summarization/summarizer.py
app/repopack-output.txt
app/test_api.py
app/tests/tempCodeRunnerFile.py
app/tests/test_awards.py
app/tests/test_certification.py
app/tests/test_cv_evaluation.py
app/tests/test_cv_summarization.py
app/tests/test_document_extraction.py
app/tests/test_education.py
app/tests/test_embedding.py
app/tests/test_experience.py
app/tests/test_scoring.py
app/utils/bias_check.py
app/utils/json_lookup.py
app/utils/normalization.py
batch_process.py
evaluation_results.json
README.md
requirements.txt
results/cv1_result.json
results/summary_report.json
test.py

================================================================
Repository Files
================================================================

================
File: .gitignore
================
.env
*.pdf

================
File: app/data/companies.json
================
{
    "FPT Software": 20,
    "VinGroup": 15,
    "Viettel": 20,
    "VNG Corporation": 15,
    "Momo": 15,
    "Shopee": 20,
    "Lazada": 20,
    "Grab": 20,
    "Google": 25,
    "Microsoft": 25,
    "Amazon": 25,
    "Startups (1-10 employees)": 5,
    "Startups (11-50 employees)": 10,
    "Startups (51-100 employees)": 15,
    "Scale-ups (101-500 employees)": 20,
    "Established Companies (501-1000 employees)": 25,
    "MNCs (1000+ employees)": 25,
    "Others": 10
  }

================
File: app/data/technical_terms.json
================
{
    "technical_majors": [
      "computer",
      "software",
      "data",
      "information",
      "cyber",
      "cloud",
      "web",
      "ai",
      "machine learning",
      "analytics",
      "engineering",
      "robotics",
      "mechatronics",
      "automation",
      "industrial",
      "mathematics",
      "statistics",
      "physics",
      "chemistry",
      "biology",
      "computational",
      "applied",
      "electrical",
      "electronics",
      "telecommunications",
      "mechanical",
      "manufacturing",
      "interaction design",
      "user experience",
      "ux",
      "civil"
    ],
    "technical_positions": [
      "software",
      "frontend",
      "backend",
      "web",
      "mobile",
      "full stack",
      "data",
      "machine learning",
      "ai",
      "devops",
      "it",
      "network",
      "cybersecurity",
      "penetration",
      "security",
      "system",
      "cloud",
      "site reliability",
      "infrastructure",
      "ci/cd",
      "hardware",
      "embedded system",
      "firmware",
      "robotics",
      "mechanical",
      "electrical",
      "automation",
      "sensor",
      "circuit",
      "product",
      "ui/ux",
      "product management",
      "design",
      "interaction",
      "experience",
      "analyst",
      "scientist",
      "engineer",
      "developer",
      "programmer",
      "specialist",
      "consultant",
      "administrator",
      "architect"
    ],
    "technical_skills": [
      "python",
      "java",
      "c++",
      "javascript",
      "react",
      "angular",
      "vue",
      "node.js",
      "sql",
      "mongodb",
      "tensorflow",
      "pytorch",
      "keras",
      "scikit-learn",
      "docker",
      "kubernetes",
      "aws",
      "azure",
      "google cloud",
      "git",
      "jenkins",
      "ansible",
      "terraform",
      "linux",
      "bash",
      "networking",
      "cybersecurity",
      "machine learning",
      "data analysis",
      "big data",
      "artificial intelligence",
      "cloud computing",
      "devops",
      "agile",
      "scrum"
    ]
  }

================
File: app/data/universities.json
================
{
    "HUST": 20,
    "RMIT University Vietnam": 20,
    "VinUniversity": 15,
    "FPT University": 15,
    "British University Vietnam (BUV)": 15,
    "University of Greenwich Vietnam": 15,
    "University of Science and Technology of Hanoi (USTH)": 15,
    "Swinburne University of Technology (Vietnam)": 15,
    "Western Sydney University Vietnam": 15,
    "Hanoi University of Science and Technology (HUST)": 20,
    "VNU University of Engineering and Technology (VNU-UET)": 20,
    "Posts and Telecommunications Institute of Technology (PTIT)": 20,
    "Ho Chi Minh City University of Technology (HCMUT)": 20,
    "Vietnam National University, Ho Chi Minh City (VNU-HCM)": 20,
    "University of Economics and Law (UEL) - VNU-HCM": 20,
    "University of Danang - University of Science and Technology": 15,
    "University of Danang - University of Science and Technology (DUT)": 15,
    "Others": 10
  }

================
File: app/main.py
================
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.responses import JSONResponse
import tempfile
import os
import json
from typing import Dict, Any
import logging
import sys
from dotenv import load_dotenv
import logging as logger
from datetime import datetime

# Update imports to use absolute imports from app root
from app.modules.document_extraction.extractor import extract_resume
from app.modules.scoring.scorer import calculate_total_score
from app.modules.summarization.summarizer import summarize_resume
from app.modules.summarization.evaluator import evaluate_resume
from app.modules.scoring.education import calculate_education_score
from app.modules.scoring.experience import calculate_experience_score
from app.modules.scoring.projects import calculate_projects_score
from app.modules.scoring.awards import calculate_awards_score
from app.modules.scoring.certifications import calculate_certifications_score

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = FastAPI()

@app.post("/api/evaluate-cv")
async def evaluate_cv_endpoint(file: UploadFile = File(...)) -> JSONResponse:
    """
    Evaluate a CV file and return detailed analysis.
    """
    try:
        # Validate file
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="File must be a PDF")

        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name

        try:
            # Extract CV data
            resume = extract_resume(temp_path)
            logger.info("Successfully extracted resume data")

            # Calculate scores
            score_result = calculate_total_score(resume)
            logger.info("Successfully calculated scores")

            # Generate summary and evaluation
            summary = summarize_resume(resume)
            evaluation = evaluate_resume(resume)
            logger.info("Successfully generated summary and evaluation")

            # Prepare response
            result = {
                "file_name": file.filename,
                "processed_at": datetime.now().isoformat(),
                "cv_data": resume.dict(),
                "scores": score_result["scores"],
                "weighted_scores": score_result["weighted_scores"],
                "status": score_result["status"],
                "total_score": score_result["total_score"],
                "ai_summary": summary,
                "ai_evaluation": evaluation
            }

            return JSONResponse(content=result)

        except Exception as e:
            logger.error(f"Processing error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Failed to process CV: {str(e)}")

        finally:
            # Clean up temp file
            try:
                os.unlink(temp_path)
            except Exception as e:
                logger.warning(f"Error removing temp file: {str(e)}")

    except Exception as e:
        logger.error(f"Error processing CV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing CV: {str(e)}")

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """
    Global exception handler for better error messages.
    """
    logger.error(f"Unhandled exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": str(exc)}
    )

# Keep existing utility functions
def get_cv_json_data(file_path: str) -> Dict[str, Any]:
    """Extract structured JSON data from a CV file."""
    try:
        resume, _ = extract_resume(file_path)
        return resume.dict()
    except Exception as e:
        print(f"Error extracting CV data: {e}")
        return {}
    

'''def evaluate_cv(file_path: str) -> Dict[str, Any]:
    """Evaluate a CV and provide detailed evaluation information."""
    try:
        resume = extract_resume(file_path)
        scoring_result = calculate_total_score(resume)
        summary = summarize_resume(resume)
        evaluation = evaluate_resume(resume)
        
        return {
            "cv_data": resume.dict(),
            "scores": {
                "education": calculate_education_score(resume.education),
                "experience": calculate_experience_score(resume.professional_experience),
                "projects": calculate_projects_score(resume.projects),
                "awards": calculate_awards_score(resume.awards),
                "certifications": calculate_certifications_score(resume.certifications),
                "total_score": scoring_result["total_score"],
                "status": scoring_result["status"]
            },
            "ai_summarization": summary,
            "ai_evaluation": evaluation
        }
    except Exception as e:
        print(f"Error evaluating CV: {e}")
        return {}'''

def export_to_json(data: Dict[str, Any], output_file: str) -> None:
    """Export evaluation data to a JSON file."""
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)
        # print(f"Evaluation data exported to {output_file}")
    except Exception as e:
        print(f"Error exporting to JSON: {e}")

'''
@app.post("/api/evaluate-cv")
async def evaluate_cv(file: UploadFile = File(...)) -> JSONResponse:
    """
    Evaluate a CV file and return detailed analysis.
    """
    try:
        # Validate file
        if not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="File must be a PDF")

        # Create temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name

        try:
            # Extract CV data
            resume = extract_resume(temp_path)
            logger.info("Successfully extracted resume data")

            # Calculate scores
            score_result = calculate_total_score(resume)
            logger.info("Successfully calculated scores")

            # Generate summary and evaluation
            summary = summarize_resume(resume)
            evaluation = evaluate_resume(resume)
            logger.info("Successfully generated summary and evaluation")

            # Prepare response
            result = {
                "file_name": file.filename,
                "processed_at": datetime.now().isoformat(),
                "cv_data": resume.dict(),
                "scores": score_result["scores"],
                "weighted_scores": score_result["weighted_scores"],
                "status": score_result["status"],
                "total_score": score_result["total_score"],
                "ai_summary": summary,
                "ai_evaluation": evaluation
            }

            return JSONResponse(content=result)

        except Exception as e:
            logger.error(f"Processing error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Failed to process CV: {str(e)}")

        finally:
            # Clean up temp file
            try:
                os.unlink(temp_path)
            except Exception as e:
                logger.warning(f"Error removing temp file: {str(e)}")

    except Exception as e:
        logger.error(f"Error processing CV: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing CV: {str(e)}")

@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    """
    Global exception handler for better error messages.
    """
    logger.error(f"Unhandled exception: {str(exc)}")
    return JSONResponse(
        status_code=500,
        content={"detail": str(exc)}
    )'''

if __name__ == "__main__":
    import uvicorn
    # Run directly from this file
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)

================
File: app/models/resume.py
================
from pydantic import BaseModel, Field
from typing import List, Optional

class EducationItem(BaseModel):
    school: str = "Unknown"
    class_year: str = "Unknown"
    major: str = ""
    minor: Optional[str] = None
    gpa: Optional[float] = None

class ProfessionalExperienceItem(BaseModel):
    company: str = "Unknown"
    location: str = ""
    position: str = "Unknown"
    seniority: str = ""
    duration: str = "Unknown"
    description: str = ""

class ProjectItem(BaseModel):
    name: str = "Unknown"
    link: Optional[str] = None
    tech: Optional[str] = None
    duration: Optional[str] = None
    description: str = ""

class AwardItem(BaseModel):
    contest: str
    prize: str
    description: str = ""  # Added default
    role: Optional[str] = None
    link: Optional[str] = None
    time: str = ""  # Added default

class CertificationItem(BaseModel):
    name: str
    link: Optional[str] = None
    org: Optional[str] = None

class SkillItem(BaseModel):
    name: str
    list: List[str] = []  # Added default

class Resume(BaseModel):
    name: str = "Unknown"  # Added default
    location: str = "Unknown"  # Added default
    social: List[str] = []  # Added default
    email: str = "Unknown"  # Added default
    linkedin: Optional[str] = None
    phone: str = "Unknown"  # Added default
    intro: str = ""  # Added default
    education: List[EducationItem] = []  # Added default
    professional_experience: List[ProfessionalExperienceItem] = []  # Added default
    projects: List[ProjectItem] = []  # Added default
    awards: List[AwardItem] = []  # Added default
    certifications: List[CertificationItem] = []  # Added default
    skills: List[SkillItem] = []  # Added default

================
File: app/models/scoring_rules.py
================
SCORING_RULES = {
    "education": {
        "weight": 15,  # 15% of the total score
        "school": 30,
        "class_year": 30,
        "major": 30,
        "minor": 0,
        "gpa": 10
    },
    "professional_experience": {
        "weight": 25,  # 25% of the total score
        "company": 25,
        "location": 5,
        "position": 25,
        "seniority": 5,
        "duration": 5,
        "description": 25
    },
    "projects": {
        "weight": 20,  # 20% of the total score
        "name": 10,
        "link": 20,
        "tech": 25,
        "duration": 5,
        "description": 40
    },
    "awards": {
        "weight": 15,  # 15% of the total score
        "contest": 30,
        "prize": 25,
        "description": 25,
        "role": 10,
        "link": 5,
        "time": 5
    },
    "certifications": {
        "weight": 5,  # 5% of the total score
        "name": 50,
        "link": 10,
        "org": 40
    },
    "skills": {
        "weight": 5,  # 5% of the total score
        "name": 50,
        "list": 50
    }
}

================
File: app/modules/document_extraction/extractor.py
================
from app.models.resume import Resume, EducationItem, ProfessionalExperienceItem, ProjectItem, AwardItem, CertificationItem, SkillItem
from .ocr import extract_structured_data_from_cv
import re
from typing import List, Optional, Dict, Any, Tuple

def extract_resume(file_path: str) -> Resume:
    """
    Extract structured data from a CV and return a Resume object.
    """
    # Extract raw text from the CV
    cv_text = extract_text_from_cv(file_path)

    # Get structured data and analysis from LLM
    result = extract_structured_data_from_cv(cv_text)
    structured_data = result["extracted_data"]
    
    # Create and return only the Resume object
    resume = Resume(
        name=structured_data.get("name", "Unknown"),
        location=structured_data.get("location", "Unknown"),
        social=structured_data.get("social", []),
        email=structured_data.get("email", "Unknown"),
        linkedin=structured_data.get("linkedin", None),
        phone=structured_data.get("phone", "Unknown"),
        intro=structured_data.get("intro", "No introduction provided."),
        education=[EducationItem(**edu) for edu in structured_data.get("education", [])],
        professional_experience=[ProfessionalExperienceItem(**exp) for exp in structured_data.get("professional_experience", [])],
        projects=[ProjectItem(**proj) for proj in structured_data.get("projects", [])],
        awards=[AwardItem(**award) for award in structured_data.get("awards", [])],
        certifications=[CertificationItem(**cert) for cert in structured_data.get("certifications", [])],
        skills=[SkillItem(**skill) for skill in structured_data.get("skills", [])]
    )
    
    return resume

def extract_text_from_cv(file_path: str) -> str:
    """
    Extract raw text from a CV file (PDF or DOCX).
    """
    if file_path.endswith(".pdf"):
        return extract_text_from_pdf(file_path)
    elif file_path.endswith(".docx"):
        return extract_text_from_docx(file_path)
    else:
        # Assume it's a plain text file
        with open(file_path, "r") as file:
            return file.read()

def extract_text_from_pdf(file_path: str) -> str:
    """
    Extract text from a PDF file.
    """
    import PyPDF2
    with open(file_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

def extract_text_from_docx(file_path: str) -> str:
    """
    Extract text from a DOCX file.
    """
    from docx import Document
    doc = Document(file_path)
    text = ""
    for paragraph in doc.paragraphs:
        text += paragraph.text + "\n"
    return text

================
File: app/modules/document_extraction/gpa_parser.py
================
import re
from typing import Optional, Dict, List
from statistics import mean

def normalize_gpa(gpa_value: str) -> Optional[float]:
    """
    Normalize GPA from various formats to a float value between 0-10 or None if invalid.
    """
    if not gpa_value or not isinstance(gpa_value, str):
        return None
        
    # Clean the input string
    gpa_value = gpa_value.strip().lower()
    
    # Case 1: Already a simple number (e.g., "3.5" or "8.7")
    try:
        gpa = float(gpa_value)
        return gpa
    except ValueError:
        pass
    
    # Case 2: Grade format with multiple years (e.g., "Grade 9: 9.9, Grade 10: 9.7, Grade 11: 9.8")
    grade_pattern = r'grade ?\d+:?\s*(\d+\.?\d*)'
    grades = re.findall(grade_pattern, gpa_value)
    if grades:
        try:
            grades = [float(grade) for grade in grades]
            return round(mean(grades), 2)
        except (ValueError, TypeError):
            pass
            
    # Case 3: Simple comma/semicolon separated grades
    try:
        # Split by comma or semicolon and clean
        grades = [g.strip() for g in re.split(r'[,;]', gpa_value)]
        # Convert to float and filter out non-numeric values
        grades = [float(g) for g in grades if re.match(r'^\d+\.?\d*$', g.strip())]
        if grades:
            return round(mean(grades), 2)
    except (ValueError, TypeError):
        pass
        
    # Case 4: Extract the first number found in the string
    number_match = re.search(r'(\d+\.?\d*)', gpa_value)
    if number_match:
        try:
            return float(number_match.group(1))
        except (ValueError, TypeError):
            pass
            
    return None

def convert_to_standard_gpa(gpa: float, scale: str = "10") -> Optional[float]:
    """
    Convert GPA to a standard 4.0 scale if needed.
    Currently supports conversion from 10-point scale.
    """
    if gpa is None:
        return None
        
    if scale == "10":
        # Convert 10-point scale to 4.0 scale
        return round((gpa * 4) / 10, 2)
    
    return gpa

def extract_and_normalize_gpa(education_data: Dict) -> Dict:
    """
    Process education data to normalize GPA values.
    """
    if "gpa" in education_data:
        raw_gpa = education_data["gpa"]
        normalized_gpa = normalize_gpa(str(raw_gpa))
        
        # Only update if we successfully normalized the GPA
        if normalized_gpa is not None:
            education_data["gpa"] = normalized_gpa
        else:
            education_data["gpa"] = None
            
    return education_data

def process_education_items(education_items: List[Dict]) -> List[Dict]:
    """
    Process a list of education items to normalize their GPA values.
    """
    return [extract_and_normalize_gpa(item) for item in education_items]

================
File: app/modules/document_extraction/ocr.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os, json, openai
from app.models.resume import Resume, EducationItem, ProfessionalExperienceItem, ProjectItem, AwardItem, CertificationItem, SkillItem
from typing import Dict, Any, List
import logging
from pathlib import Path
import PyPDF2
from pydantic import ValidationError

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

logger = logging.getLogger(__name__)

def extract_resume(file_path: str) -> Resume:
    """
    Extract resume data from a PDF file and return a validated Resume object.
    """
    try:
        # Extract text from PDF
        cv_text = extract_text_from_cv(file_path)
        
        # Extract structured data
        structured_data = extract_structured_data_from_cv(cv_text)
        
        # Create Resume object with proper validation
        try:
            resume = Resume(
                name=structured_data.get("name", "Unknown"),
                location=structured_data.get("location", "Unknown"),
                social=structured_data.get("social", []),
                email=structured_data.get("email", "Unknown"),
                linkedin=structured_data.get("linkedin"),
                phone=structured_data.get("phone", "Unknown"),
                intro=structured_data.get("intro", ""),
                education=[
                    EducationItem(
                        school=edu.get("school", "Unknown"),
                        class_year=edu.get("class_year", "Unknown"),
                        major=edu.get("major", ""),
                        minor=edu.get("minor"),
                        gpa=float(edu["gpa"]) if edu.get("gpa") and str(edu["gpa"]).replace(".", "").isdigit() else None
                    ) 
                    for edu in structured_data.get("education", [])
                ],
                professional_experience=[
                    ProfessionalExperienceItem(
                        company=exp.get("company", "Unknown"),
                        location=exp.get("location", ""),
                        position=exp.get("position", "Unknown"),
                        seniority=exp.get("seniority", ""),
                        duration=exp.get("duration", "Unknown"),
                        description=exp.get("description", "")
                    )
                    for exp in structured_data.get("professional_experience", [])
                ],
                projects=[
                    ProjectItem(
                        name=proj.get("name", "Unknown"),
                        link=proj.get("link"),
                        tech=proj.get("tech"),
                        duration=proj.get("duration"),
                        description=proj.get("description", "")
                    )
                    for proj in structured_data.get("projects", [])
                ],
                awards=[
                    AwardItem(
                        contest=award.get("contest", "Unknown"),
                        prize=award.get("prize", "Unknown"),
                        description=award.get("description", ""),
                        role=award.get("role"),
                        link=award.get("link"),
                        time=award.get("time", "")
                    )
                    for award in structured_data.get("awards", [])
                ],
                certifications=[
                    CertificationItem(
                        name=cert.get("name", "Unknown"),
                        link=cert.get("link"),
                        org=cert.get("org")
                    )
                    for cert in structured_data.get("certifications", [])
                ],
                skills=[
                    SkillItem(
                        name=skill.get("name", "Unknown"),
                        list=skill.get("list", [])
                    )
                    for skill in structured_data.get("skills", [])
                ]
            )
            return resume
            
        except ValidationError as e:
            logger.error(f"Validation error creating Resume object: {str(e)}")
            # Return a default Resume object instead of raising an error
            return Resume()
            
    except Exception as e:
        logger.error(f"Error in extract_resume: {str(e)}")
        # Return a default Resume object instead of raising an error
        return Resume()

def extract_text_from_cv(file_path: str) -> str:
    """
    Extract text from PDF file with enhanced error handling.
    """
    try:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
            
        if not file_path.lower().endswith('.pdf'):
            raise ValueError("File must be a PDF")
            
        with open(file_path, "rb") as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page in reader.pages:
                try:
                    text += page.extract_text() + "\n"
                except Exception as e:
                    logger.warning(f"Error extracting text from page: {str(e)}")
                    continue
                    
            if not text.strip():
                raise ValueError("No text could be extracted from the PDF")
                
            return text
                
    except Exception as e:
        logger.error(f"Error extracting text from PDF: {str(e)}")
        raise

def create_default_structure() -> Dict[str, Any]:
    """
    Creates a default structure for the resume data.
    """
    return {
        "extracted_data": {
            "name": "Unknown",
            "location": "Unknown",
            "social": [],
            "email": "Unknown",
            "linkedin": None,
            "phone": "Unknown",
            "intro": "No introduction provided.",
            "education": [],
            "professional_experience": [],
            "projects": [],
            "awards": [],
            "certifications": [],
            "skills": [{"name": "Unknown", "list": []}]
        },
        "summary": "Unable to generate summary.",
        "evaluation": "Unable to generate evaluation.",
        "scoring_recommendations": {
            "education": {"score": 0, "reasoning": "Unable to evaluate"},
            "experience": {"score": 0, "reasoning": "Unable to evaluate"},
            "projects": {"score": 0, "reasoning": "Unable to evaluate"},
            "awards": {"score": 0, "reasoning": "Unable to evaluate"},
            "certifications": {"score": 0, "reasoning": "Unable to evaluate"}
        }
    }

def extract_structured_data_from_cv(cv_text: str) -> Dict[str, Any]:
    """
    Extract structured data, summary, and evaluation from CV text using a single OpenAI API call.
    """
    client = OpenAI()
    
    if not cv_text.strip():
        raise ValueError("Empty CV text provided")
    
    # Clean the CV text by removing any markdown-style code block markers.
    cleaned_cv_text = cv_text.replace("```", "").strip()
    
    prompt = f"""Analyze the following CV and provide:
1. Structured data extraction
2. Brief summary
3. Detailed evaluation
4. Initial scoring recommendations

Important:
- If a field is empty or not found, use an empty string "" for text fields and [] for lists.
- Never return null/None for required fields.
- For dates, if format is unclear, return as string in the original format found.

Return the results in the following JSON format exactly:
{{
    "extracted_data": {{
        "name": "string",
        "location": "string",
        "social": ["string"],
        "email": "string",
        "linkedin": "string or null",
        "phone": "string",
        "intro": "string",
        "education": [{{
            "school": "string",
            "class_year": "string",
            "major": "string",
            "gpa": "number or null"
        }}],
        "professional_experience": [{{
            "company": "string",
            "location": "string",
            "position": "string",
            "seniority": "string",
            "duration": "string",
            "description": "string"
        }}],
        "projects": [{{
            "name": "string",
            "tech": "string",
            "duration": "string",
            "description": "string"
        }}],
        "awards": [{{
            "contest": "string",
            "prize": "string",
            "description": "string",
            "time": "string"
        }}],
        "certifications": [{{
            "name": "string",
            "org": "string",
            "link": "string"
        }}],
        "skills": [{{
            "name": "string",
            "list": ["string"]
        }}]
    }},
    "summary": "string",
    "evaluation": "string",
    "scoring_recommendations": {{
        "education": {{"score": "number", "reasoning": "string"}},
        "experience": {{"score": "number", "reasoning": "string"}},
        "projects": {{"score": "number", "reasoning": "string"}},
        "awards": {{"score": "number", "reasoning": "string"}},
        "certifications": {{"score": "number", "reasoning": "string"}}
    }}
}}

CV Text:
{cleaned_cv_text}
"""
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a professional CV parser. Return only valid JSON that matches the required structure exactly."
                },
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.3
        )
        
        response_content = response.choices[0].message.content.strip()
        # Remove markdown formatting markers if present.
        response_content = response_content.replace("```json", "").replace("```", "").strip()
        
        if not response_content:
            logger.error("Empty response from API")
            return create_default_structure()
        
        try:
            structured_data = json.loads(response_content)
            if not isinstance(structured_data, dict) or "extracted_data" not in structured_data:
                logger.error("Response JSON missing required 'extracted_data' field")
                return create_default_structure()
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON response: {e}")
            logger.error(f"Raw response: {response_content}")
            return create_default_structure()
            
    except Exception as e:
        logger.error(f"Error during API call: {e}")
        return create_default_structure()
    
    # Ensure all required fields exist.
    default_structure = create_default_structure()
    for key in default_structure:
        if key not in structured_data:
            structured_data[key] = default_structure[key]
    
    return structured_data

================
File: app/modules/embedding/embedder.py
================
from transformers import AutoTokenizer, AutoModel
import torch
from typing import List


def get_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Generate embeddings for a list of texts using PhoBERT.
    """
    tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
    model = AutoModel.from_pretrained("vinai/phobert-base")

    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).tolist()
    return embeddings

================
File: app/modules/embedding/similarity.py
================
from typing import List
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(embedding1: List[float], embedding2: List[float]) -> float:
    """
    Calculate cosine similarity between two embeddings.
    """
    return cosine_similarity([embedding1], [embedding2])[0][0]

================
File: app/modules/scoring/awards.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import AwardItem
from app.models.scoring_rules import SCORING_RULES
from typing import List, Optional

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def infer_contest_prestige(contest: str) -> int:
    """
    Use LLM to infer contest prestige if not found in the JSON file.
    """
    client = OpenAI(api_key=openai_api_key)
    
    prompt = f"""
    Based on the following contest name, provide a prestige score between 0 and 30, where 30 is the highest.
    Contest: {contest}
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a contest prestige evaluator. Provide a score between 0 and 30."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=10,
        temperature=0.3
    )
    
    try:
        score = int(response.choices[0].message.content.strip())
        return min(max(score, 0), 30)  # Ensure score is between 0 and 30
    except:
        return 10  # Default score if parsing fails

def calculate_awards_score(awards: List[AwardItem]) -> float:
    """
    Calculate the awards score with LLM fallback for unknown contests.
    """
    total_score = 0.0

    for award in awards:
        # Contest Prestige (30%)
        contest_score = infer_contest_prestige(award.contest) * SCORING_RULES["awards"]["contest"]
        
        # Prize (25%)
        prize_score = calculate_prize_score(award.prize) * SCORING_RULES["awards"]["prize"]
        
        # Description (25%)
        description_score = calculate_description_score(award.description) * SCORING_RULES["awards"]["description"]
        
        # Role (10%)
        role_score = 10 if is_technical_role(award.role) else 0
        
        # Link (5%)
        link_score = 5 if award.link else 0
        
        # Time (5%)
        time_score = 5 if award.time else 0
        
        # Total Award Score
        total_score += contest_score + prize_score + description_score + role_score + link_score + time_score

    return total_score

def calculate_prize_score(prize: str) -> int:
    """
    Calculate the score based on the prize level.
    """
    if "1st" in prize.lower() or "gold" in prize.lower():
        return 25
    elif "2nd" in prize.lower() or "silver" in prize.lower():
        return 20
    elif "3rd" in prize.lower() or "bronze" in prize.lower():
        return 15
    else:
        return 10

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity and impact.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

def is_technical_role(role: Optional[str]) -> bool:
    """
    Check if the role is relevant to technical fields.
    """
    if not role:
        return False
    technical_keywords = ["developer", "engineer", "data scientist", "analyst"]
    return any(keyword in role.lower() for keyword in technical_keywords)

================
File: app/modules/scoring/certifications.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import CertificationItem
from app.models.scoring_rules import SCORING_RULES
from typing import List, Optional

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def infer_certification_relevance(name: str) -> int:
    """
    Use LLM to infer certification relevance if not found in the JSON file.
    """
    client = OpenAI(api_key=openai_api_key)
    
    prompt = f"""
    Based on the following certification name, provide a relevance score between 0 and 50, where 50 is the highest.
    Certification: {name}
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a certification relevance evaluator. Provide a score between 0 and 50."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=10,
        temperature=0.3
    )
    
    try:
        score = int(response.choices[0].message.content.strip())
        return min(max(score, 0), 50)  # Ensure score is between 0 and 50
    except:
        return 10  # Default score if parsing fails

def calculate_certifications_score(certifications: List[CertificationItem]) -> float:
    """
    Calculate the certifications score with LLM fallback for unknown certifications.
    """
    total_score = 0.0

    for cert in certifications:
        # Get name score from JSON or LLM fallback
        name_score = calculate_name_score(cert.name)
        if name_score == 10:  # Default score for unknown certifications
            name_score = infer_certification_relevance(cert.name)
        
        name_score = name_score * SCORING_RULES["certifications"]["name"]
        org_score = calculate_org_score(cert.org) * SCORING_RULES["certifications"]["org"]
        link_score = 10 if cert.link else 0
        
        total_score += name_score + org_score + link_score

    return total_score

def calculate_name_score(name: str) -> int:
    """
    Calculate the score based on the certification name's relevance.
    """
    if "aws" in name.lower() or "google" in name.lower() or "microsoft" in name.lower():
        return 50
    elif "data" in name.lower() or "cloud" in name.lower() or "ai" in name.lower():
        return 30
    else:
        return 10

def calculate_org_score(org: Optional[str]) -> int:
    """
    Calculate the score based on the organization's reputation.
    """
    if not org:
        return 0
    if "aws" in org.lower() or "google" in org.lower() or "microsoft" in org.lower():
        return 40
    elif "udemy" in org.lower() or "coursera" in org.lower():
        return 25
    else:
        return 10

================
File: app/modules/scoring/education.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import EducationItem
from app.models.scoring_rules import SCORING_RULES
from app.utils.json_lookup import get_university_score
from typing import List

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def infer_university_reputation(university: str) -> int:
    """
    Use LLM to infer university reputation if not found in the JSON file.
    """
    client = OpenAI(api_key=openai_api_key)
    
    prompt = f"""
    Based on the following university name, provide a reputation score between 0 and 20, where 20 is the highest reputation.
    University: {university}
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a university reputation evaluator. Provide a score between 0 and 20."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=10,
        temperature=0.3
    )
    
    try:
        score = int(response.choices[0].message.content.strip())
        return min(max(score, 0), 20)  # Ensure score is between 0 and 20
    except:
        return 10  # Default score if parsing fails

def calculate_education_score(education_items: List[EducationItem]) -> float:
    """
    Calculate education score out of 100 points based on PRD criteria.
    
    Breakdown:
    - School (30%): Based on university tier
    - Class Year (30%): Based on graduation year
    - Major (30%): Based on relevance to technical field
    - GPA (10%): Based on GPA thresholds
    """
    total_score = 0.0
    max_edu_score = 0.0  # Track highest scoring education item

    for edu in education_items:
        # School Score (30 points)
        school_score = get_university_score(edu.school) * 0.3  # Normalize to 30 points
        
        # Class Year Score (30 points)
        year_score = calculate_class_score(edu.class_year)
        
        # Major Score (30 points)
        major_score = 30.0 if is_technical_major(edu.major) else 0.0
        
        # GPA Score (10 points)
        gpa_score = 0.0
        if edu.gpa:
            if edu.gpa >= 3.2:
                gpa_score = 10.0
            elif edu.gpa >= 2.8:
                gpa_score = 5.0

        # Calculate total score for this education item
        edu_score = school_score + year_score + major_score + gpa_score
        
        # Keep highest scoring education
        max_edu_score = max(max_edu_score, edu_score)

    return max_edu_score

def is_technical_major(major: str) -> bool:
    """
    Check if the major is relevant to technical fields.
    """
    technical_keywords = ["computer", "software", "data", "information", "cyber", "cloud", "web", "ai", "machine learning", "analytics"]
    return any(keyword in major.lower() for keyword in technical_keywords)

def calculate_class_score(class_year: str) -> int:
    """
    Calculate the score based on the class year.
    """
    if class_year.lower() == "senior":
        return 30
    elif class_year.lower() == "junior":
        return 20
    elif class_year.lower() == "sophomore":
        return 10
    else:
        return 10

================
File: app/modules/scoring/experience.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import ProfessionalExperienceItem
from app.models.scoring_rules import SCORING_RULES
from app.utils.json_lookup import get_company_score
from typing import List

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def infer_company_size(company: str) -> int:
    """
    Use LLM to infer company size if not found in the JSON file.
    """
    client = OpenAI(api_key=openai_api_key)
    
    prompt = f"""
    Based on the following company name, provide a size score between 0 and 25, where 25 is the highest.
    Company: {company}
    """
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "You are a company size evaluator. Provide a score between 0 and 25."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=10,
        temperature=0.3
    )
    
    try:
        score = int(response.choices[0].message.content.strip())
        return min(max(score, 0), 25)  # Ensure score is between 0 and 25
    except:
        return 10  # Default score if parsing fails

def calculate_experience_score(experience: List[ProfessionalExperienceItem]) -> float:
    """
    Calculate the experience score with LLM fallback for unknown companies.
    """
    total_score = 0.0

    for exp in experience:
        # Get company score from JSON or LLM fallback
        company_score = get_company_score(exp.company)
        if company_score == 10:  # Default score for unknown companies
            company_score = infer_company_size(exp.company)
        
        # Rest of the scoring logic
        company_score = company_score * SCORING_RULES["professional_experience"]["company"]
        position_score = 25 if is_technical_position(exp.position) else 0
        description_score = calculate_description_score(exp.description)
        seniority_score = 5 if exp.seniority.lower() in ["senior", "lead", "manager"] else 0
        duration_score = 5 if calculate_duration(exp.duration) >= 6 else 0
        
        total_score += company_score + position_score + description_score + seniority_score + duration_score

    return total_score

def is_technical_position(position: str) -> bool:
    """
    Check if the position is relevant to technical roles.
    """
    technical_keywords = ["software", "frontend", "backend", "web", "mobile", "full stack", "data", "machine learning", "ai", "devops"]
    return any(keyword in position.lower() for keyword in technical_keywords)

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity, impact, and use of tools.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

def calculate_duration(duration: str) -> int:
    """
    Calculate the duration in months.
    """
    try:
        if " - " in duration:
            # Format: "Jan 2020 - Dec 2021"
            start, end = duration.split(" - ")
            start_year = int(start.split()[-1])
            end_year = int(end.split()[-1])
            return (end_year - start_year) * 12
        elif "year" in duration:
            # Format: "2 years"
            years = int(duration.split()[0])
            return years * 12
        elif "month" in duration:
            # Format: "6 months"
            months = int(duration.split()[0])
            return months
        else:
            # Default to 0 if the format is unrecognized
            return 0
    except:
        # If parsing fails, return 0
        return 0

================
File: app/modules/scoring/projects.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import ProjectItem
from app.models.scoring_rules import SCORING_RULES
from typing import List, Optional

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

def infer_tech_stack_relevance(tech: str) -> int:
    """
    Use LLM to infer tech stack relevance if not found in the JSON file.
    """
    client = OpenAI(api_key=openai_api_key)
    
    prompt = f"""
    Based on the following tech stack, provide a relevance score between 0 and 25, where 25 is the highest.
    Tech Stack: {tech}
    """
    
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "You are a tech stack relevance evaluator. Provide a score between 0 and 25."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=10,
        temperature=0.3
    )
    
    try:
        score = int(response.choices[0].message.content.strip())
        return min(max(score, 0), 25)  # Ensure score is between 0 and 25
    except:
        return 10  # Default score if parsing fails

def calculate_projects_score(projects: List[ProjectItem]) -> float:
    """
    Calculate the projects score with LLM fallback for unknown tech stacks.
    """
    total_score = 0.0

    for project in projects:
        # Tech Stack (25%)
        tech_score = infer_tech_stack_relevance(project.tech) * SCORING_RULES["projects"]["tech"]
        
        # GitHub Link (20%)
        link_score = 20 if project.link else 0
        
        # Description (40%)
        description_score = calculate_description_score(project.description) * SCORING_RULES["projects"]["description"]
        
        # Total Project Score
        total_score += tech_score + link_score + description_score

    return total_score

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity, impact, and use of tools.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

================
File: app/modules/scoring/scorer.py
================
from typing import Dict, Any
import logging
from app.models.resume import Resume
from app.models.scoring_rules import SCORING_RULES
from app.modules.scoring.education import calculate_education_score
from app.modules.scoring.experience import calculate_experience_score
from app.modules.scoring.projects import calculate_projects_score
from app.modules.scoring.awards import calculate_awards_score
from app.modules.scoring.certifications import calculate_certifications_score

logger = logging.getLogger(__name__)

def calculate_total_score(resume: Resume) -> Dict[str, Any]:
    """
    Calculate the total score for a resume.
    Each category's raw score is calculated out of 100 points, then weighted according to PRD.
    
    Weights from PRD:
    - Education: 15%
    - Professional Experience: 25%
    - Projects: 20%
    - Awards: 15%
    - Certifications: 5%
    """
    try:
        # Calculate raw scores (each out of 100)
        raw_scores = {
            "education": calculate_education_score(resume.education),
            "experience": calculate_experience_score(resume.professional_experience),
            "projects": calculate_projects_score(resume.projects),
            "awards": calculate_awards_score(resume.awards),
            "certifications": calculate_certifications_score(resume.certifications)
        }

        # Apply weights
        weighted_scores = {
            "education": raw_scores["education"] * (SCORING_RULES["education"]["weight"] / 100),
            "experience": raw_scores["experience"] * (SCORING_RULES["professional_experience"]["weight"] / 100),
            "projects": raw_scores["projects"] * (SCORING_RULES["projects"]["weight"] / 100),
            "awards": raw_scores["awards"] * (SCORING_RULES["awards"]["weight"] / 100),
            "certifications": raw_scores["certifications"] * (SCORING_RULES["certifications"]["weight"] / 100)
        }

        # Calculate total score
        total_score = sum(weighted_scores.values())

        # Determine status based on total score
        status = "Pass" if total_score >= 70 else "Consider" if total_score >= 50 else "Fail"

        return {
            "scores": raw_scores,  # Raw scores out of 100
            "weighted_scores": weighted_scores,  # Scores after applying weights
            "status": status,
            "total_score": total_score
        }

    except Exception as e:
        logger.error(f"Error calculating total score: {str(e)}")
        raise

================
File: app/modules/summarization/evaluator.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import Resume

load_dotenv()

def evaluate_resume(resume: Resume) -> str:
    """
    Evaluate a resume using OpenAI's API.
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    prompt = f"Please evaluate the following resume:\n{resume}"
    
    response = client.chat.completions.create(
        model="gpt-4o",  
        messages=[
            {"role": "system", "content": "You are a professional resume evaluator. Provide a detailed evaluation of the resume."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=1000,
        temperature=0.7
    )
    
    evaluation = response.choices[0].message.content
    
    return evaluation

def format_resume_for_evaluation(resume: Resume) -> str:
    """
    Format the resume into a text string for evaluation.
    """
    evaluation = f"Name: {resume.name}\n"
    evaluation += f"Location: {resume.location}\n"
    evaluation += f"Email: {resume.email}\n"
    evaluation += f"LinkedIn: {resume.linkedin}\n"
    evaluation += f"Phone: {resume.phone}\n"
    evaluation += f"Intro: {resume.intro}\n"

    evaluation += "\nEducation:\n"
    for edu in resume.education:
        evaluation += f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}\n"

    evaluation += "\nProfessional Experience:\n"
    for exp in resume.professional_experience:
        evaluation += f"- {exp.company}, {exp.position}, {exp.duration}\n"
        evaluation += f"  {exp.description}\n"

    evaluation += "\nProjects:\n"
    for project in resume.projects:
        evaluation += f"- {project.name}, {project.tech}, {project.duration}\n"
        evaluation += f"  {project.description}\n"

    evaluation += "\nAwards:\n"
    for award in resume.awards:
        evaluation += f"- {award.contest}, {award.prize}, {award.time}\n"
        evaluation += f"  {award.description}\n"

    evaluation += "\nCertifications:\n"
    for cert in resume.certifications:
        evaluation += f"- {cert.name}, {cert.org}\n"

    evaluation += "\nSkills:\n"
    for skill in resume.skills:
        evaluation += f"- {skill.name}: {', '.join(skill.list)}\n"

    return evaluation

================
File: app/modules/summarization/summarizer.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import Resume

load_dotenv()

def summarize_resume(resume: Resume) -> str:
    """
    Summarize a resume using OpenAI's API.
    """
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    prompt = f"Please summarize the following resume:\n{resume}"
    
    response = client.chat.completions.create(
        model="gpt-4o",  
        messages=[
            {"role": "system", "content": "You are a professional resume reviewer. Summarize the key points of the resume concisely."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500,
        temperature=0.7
    )
    
    summary = response.choices[0].message.content
    
    return summary

def format_resume_for_summary(resume: Resume) -> str:
    """
    Format the resume into a text string for summarization.
    """
    summary = f"Name: {resume.name}\n"
    summary += f"Location: {resume.location}\n"
    summary += f"Email: {resume.email}\n"
    summary += f"LinkedIn: {resume.linkedin}\n"
    summary += f"Phone: {resume.phone}\n"
    summary += f"Intro: {resume.intro}\n"

    summary += "\nEducation:\n"
    for edu in resume.education:
        summary += f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}\n"

    summary += "\nProfessional Experience:\n"
    for exp in resume.professional_experience:
        summary += f"- {exp.company}, {exp.position}, {exp.duration}\n"
        summary += f"  {exp.description}\n"

    summary += "\nProjects:\n"
    for project in resume.projects:
        summary += f"- {project.name}, {project.tech}, {project.duration}\n"
        summary += f"  {project.description}\n"

    summary += "\nAwards:\n"
    for award in resume.awards:
        summary += f"- {award.contest}, {award.prize}, {award.time}\n"
        summary += f"  {award.description}\n"

    summary += "\nCertifications:\n"
    for cert in resume.certifications:
        summary += f"- {cert.name}, {cert.org}\n"

    summary += "\nSkills:\n"
    for skill in resume.skills:
        summary += f"- {skill.name}: {', '.join(skill.list)}\n"

    return summary

================
File: app/repopack-output.txt
================
================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2025-01-23T06:07:05.427Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
data/companies.json
data/technical_terms.json
data/universities.json
models/resume.py
models/scoring_rules.py
modules/document_extraction/extractor.py
modules/document_extraction/ocr.py
modules/embedding/embedder.py
modules/embedding/similarity.py
modules/scoring/awards.py
modules/scoring/certifications.py
modules/scoring/education.py
modules/scoring/experience.py
modules/scoring/projects.py
modules/scoring/scorer.py
modules/summarization/evaluator.py
modules/summarization/summarizer.py
tests/tempCodeRunnerFile.py
tests/test_awards.py
tests/test_certification.py
tests/test_cv_evaluation.py
tests/test_cv_summarization.py
tests/test_document_extraction.py
tests/test_education.py
tests/test_embedding.py
tests/test_experience.py
tests/test_scoring.py
utils/bias_check.py
utils/json_lookup.py
utils/normalization.py

================================================================
Repository Files
================================================================

================
File: data/companies.json
================
{
    "FPT Software": 20,
    "VinGroup": 15,
    "Viettel": 20,
    "VNG Corporation": 15,
    "Momo": 15,
    "Shopee": 20,
    "Lazada": 20,
    "Grab": 20,
    "Google": 25,
    "Microsoft": 25,
    "Amazon": 25,
    "Startups (1-10 employees)": 5,
    "Startups (11-50 employees)": 10,
    "Startups (51-100 employees)": 15,
    "Scale-ups (101-500 employees)": 20,
    "Established Companies (501-1000 employees)": 25,
    "MNCs (1000+ employees)": 25,
    "Others": 10
  }

================
File: data/technical_terms.json
================
{
    "technical_majors": [
      "computer",
      "software",
      "data",
      "information",
      "cyber",
      "cloud",
      "web",
      "ai",
      "machine learning",
      "analytics",
      "engineering",
      "robotics",
      "mechatronics",
      "automation",
      "industrial",
      "mathematics",
      "statistics",
      "physics",
      "chemistry",
      "biology",
      "computational",
      "applied",
      "electrical",
      "electronics",
      "telecommunications",
      "mechanical",
      "manufacturing",
      "interaction design",
      "user experience",
      "ux",
      "civil"
    ],
    "technical_positions": [
      "software",
      "frontend",
      "backend",
      "web",
      "mobile",
      "full stack",
      "data",
      "machine learning",
      "ai",
      "devops",
      "it",
      "network",
      "cybersecurity",
      "penetration",
      "security",
      "system",
      "cloud",
      "site reliability",
      "infrastructure",
      "ci/cd",
      "hardware",
      "embedded system",
      "firmware",
      "robotics",
      "mechanical",
      "electrical",
      "automation",
      "sensor",
      "circuit",
      "product",
      "ui/ux",
      "product management",
      "design",
      "interaction",
      "experience",
      "analyst",
      "scientist",
      "engineer",
      "developer",
      "programmer",
      "specialist",
      "consultant",
      "administrator",
      "architect"
    ],
    "technical_skills": [
      "python",
      "java",
      "c++",
      "javascript",
      "react",
      "angular",
      "vue",
      "node.js",
      "sql",
      "mongodb",
      "tensorflow",
      "pytorch",
      "keras",
      "scikit-learn",
      "docker",
      "kubernetes",
      "aws",
      "azure",
      "google cloud",
      "git",
      "jenkins",
      "ansible",
      "terraform",
      "linux",
      "bash",
      "networking",
      "cybersecurity",
      "machine learning",
      "data analysis",
      "big data",
      "artificial intelligence",
      "cloud computing",
      "devops",
      "agile",
      "scrum"
    ]
  }

================
File: data/universities.json
================
{
    "HUST": 20,
    "RMIT University Vietnam": 20,
    "VinUniversity": 15,
    "FPT University": 15,
    "British University Vietnam (BUV)": 15,
    "University of Greenwich Vietnam": 15,
    "University of Science and Technology of Hanoi (USTH)": 15,
    "Swinburne University of Technology (Vietnam)": 15,
    "Western Sydney University Vietnam": 15,
    "Hanoi University of Science and Technology (HUST)": 20,
    "VNU University of Engineering and Technology (VNU-UET)": 20,
    "Posts and Telecommunications Institute of Technology (PTIT)": 20,
    "Ho Chi Minh City University of Technology (HCMUT)": 20,
    "Vietnam National University, Ho Chi Minh City (VNU-HCM)": 20,
    "University of Economics and Law (UEL) - VNU-HCM": 20,
    "University of Danang - University of Science and Technology": 15,
    "University of Danang - University of Science and Technology (DUT)": 15,
    "Others": 10
  }

================
File: models/resume.py
================
from pydantic import BaseModel, Field
from typing import List, Optional

class EducationItem(BaseModel):
    school: str
    class_year: str
    major: str
    minor: Optional[str] = None
    gpa: Optional[float] = None

class ProfessionalExperienceItem(BaseModel):
    company: str
    location: str
    position: str
    seniority: str
    duration: str
    description: str

class ProjectItem(BaseModel):
    name: str
    link: Optional[str] = None
    tech: Optional[str] = None
    duration: Optional[str] = None
    description: str

class AwardItem(BaseModel):
    contest: str
    prize: str
    description: str
    role: Optional[str] = None
    link: Optional[str] = None
    time: str

class CertificationItem(BaseModel):
    name: str
    link: Optional[str] = None
    org: Optional[str] = None

class SkillItem(BaseModel):
    name: str
    list: List[str]

class Resume(BaseModel):
    name: str
    location: str
    social: List[str]
    email: str
    linkedin: Optional[str] = None
    phone: str
    intro: str
    education: List[EducationItem]
    professional_experience: List[ProfessionalExperienceItem]
    projects: List[ProjectItem]
    awards: List[AwardItem]
    certifications: List[CertificationItem]
    skills: List[SkillItem]

================
File: models/scoring_rules.py
================
SCORING_RULES = {
    "education": {
        "weight": 15,  # 15% of the total score
        "school": 30,
        "class_year": 30,
        "major": 30,
        "minor": 0,
        "gpa": 10
    },
    "professional_experience": {
        "weight": 25,  # 25% of the total score
        "company": 25,
        "location": 5,
        "position": 25,
        "seniority": 5,
        "duration": 5,
        "description": 25
    },
    "projects": {
        "weight": 20,  # 20% of the total score
        "name": 10,
        "link": 20,
        "tech": 25,
        "duration": 5,
        "description": 40
    },
    "awards": {
        "weight": 15,  # 15% of the total score
        "contest": 30,
        "prize": 25,
        "description": 25,
        "role": 10,
        "link": 5,
        "time": 5
    },
    "certifications": {
        "weight": 5,  # 5% of the total score
        "name": 50,
        "link": 10,
        "org": 40
    },
    "skills": {
        "weight": 5,  # 5% of the total score
        "name": 50,
        "list": 50
    }
}

================
File: modules/document_extraction/extractor.py
================
from app.models.resume import Resume, EducationItem, ProfessionalExperienceItem, ProjectItem, AwardItem, CertificationItem, SkillItem
from .ocr import extract_structured_data_from_cv
import re
from typing import List, Optional

def extract_resume(file_path: str) -> Resume:
    """
    Extract structured data from a CV (PDF or DOCX) using LLM and return a Resume object.
    """
    # Step 1: Extract raw text from the CV
    cv_text = extract_text_from_cv(file_path)

    # Step 2: Use LLM to extract structured data
    structured_data = extract_structured_data_from_cv(cv_text)

    print(structured_data)
    
    # Step 3: Convert the structured data into a Resume object
    return Resume(
        name=structured_data.get("name", "Unknown"),
        location=structured_data.get("location", "Unknown"),
        social=structured_data.get("social", []),
        email=structured_data.get("email", "Unknown"),
        linkedin=structured_data.get("linkedin", None),
        phone=structured_data.get("phone", "Unknown"),
        intro=structured_data.get("intro", "No introduction provided."),
        education=[EducationItem(**edu) for edu in structured_data.get("education", [])],
        professional_experience=[ProfessionalExperienceItem(**exp) for exp in structured_data.get("professional_experience", [])],
        projects=[ProjectItem(**proj) for proj in structured_data.get("projects", [])],
        awards=[AwardItem(**award) for award in structured_data.get("awards", [])],
        certifications=[CertificationItem(**cert) for cert in structured_data.get("certifications", [])],
        skills=[SkillItem(**skill) for skill in structured_data.get("skills", [])]
    )

def extract_text_from_cv(file_path: str) -> str:
    """
    Extract raw text from a CV file (PDF or DOCX).
    """
    if file_path.endswith(".pdf"):
        return extract_text_from_pdf(file_path)
    elif file_path.endswith(".docx"):
        return extract_text_from_docx(file_path)
    else:
        # Assume it's a plain text file
        with open(file_path, "r") as file:
            return file.read()

def extract_text_from_pdf(file_path: str) -> str:
    """
    Extract text from a PDF file.
    """
    import PyPDF2
    with open(file_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

def extract_text_from_docx(file_path: str) -> str:
    """
    Extract text from a DOCX file.
    """
    from docx import Document
    doc = Document(file_path)
    text = ""
    for paragraph in doc.paragraphs:
        text += paragraph.text + "\n"
    return text

================
File: modules/document_extraction/ocr.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os, json, openai, re

# Load environment variables
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

def extract_structured_data_from_cv(cv_text: str) -> dict:
    """
    Extract structured data from CV text using OpenAI's API.
    Returns a dictionary containing structured resume data.
    """
    # Initialize the OpenAI client
    client = OpenAI()
    
    # Create the prompt from the CV text
    prompt = f"""
    Extract the following fields from the CV below and return the result as a valid JSON object. Do not include any additional text or explanations. Only return the JSON object.

    Required JSON structure:
    {{
        "name": "string",
        "location": "string",
        "social": ["string"],
        "email": "string",
        "linkedin": "string or null",
        "phone": "string",
        "intro": "string",
        "education": [{{
            "school": "string",
            "class_year": "string",
            "major": "string",
            "gpa": "string"
        }}],
        "professional_experience": [{{
            "company": "string",
            "location": "string",
            "position": "string",
            "seniority": "string",
            "duration": "string",
            "description": "string"
        }}],
        "projects": [{{
            "name": "string",
            "tech": "string",
            "duration": "string",
            "description": "string"
        }}],
        "awards": [{{
            "contest": "string",
            "prize": "string",
            "description": "string",
            "time": "string"
        }}],
        "certifications": [{{
            "name": "string",
            "org": "string",
            "link": "string"
        }}],
        "skills": [{{
            "name": "string",
            "list": ["string"]
        }}]
    }}

    CV Text:
    {cv_text}"""
    
    try:
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {
                    "role": "system", 
                    "content": "You are a professional CV parser. Return only valid JSON that matches the required structure exactly."
                },
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.3
        )
        
        response_content = response.choices[0].message.content.strip()
        if not response_content:
            print("Empty response from API")
            return create_default_structure()
            
        try:
            structured_data = json.loads(response_content)
            if not isinstance(structured_data, dict):
                print("Response is not a dictionary")
                return create_default_structure()
                
            # Fix skills structure if needed
            if "skills" in structured_data:
                for skill in structured_data["skills"]:
                    if "skills" in skill and "list" not in skill:
                        skill["list"] = skill.pop("skills")
                    elif "list" not in skill:
                        skill["list"] = []
                        
        except json.JSONDecodeError as e:
            print(f"Failed to parse JSON response: {e}")
            print(f"Raw response: {response_content}")
            return create_default_structure()
            
    except Exception as e:
        print(f"Error during API call: {e}")
        return create_default_structure()
    
    # Ensure all required fields exist
    default_structure = create_default_structure()
    for key in default_structure:
        if key not in structured_data:
            structured_data[key] = default_structure[key]
    
    return structured_data

def create_default_structure() -> dict:
    """
    Creates a default structure for the resume data.
    """
    return {
        "name": "Unknown",
        "location": "Unknown",
        "social": [],
        "email": "Unknown",
        "linkedin": None,
        "phone": "Unknown",
        "intro": "No introduction provided.",
        "education": [],
        "professional_experience": [],
        "projects": [],
        "awards": [],
        "certifications": [],
        "skills": [{"name": "Unknown", "list": []}]
    }

================
File: modules/embedding/embedder.py
================
from transformers import AutoTokenizer, AutoModel
import torch
from typing import List


def get_embeddings(texts: List[str]) -> List[List[float]]:
    """
    Generate embeddings for a list of texts using PhoBERT.
    """
    tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base")
    model = AutoModel.from_pretrained("vinai/phobert-base")

    inputs = tokenizer(texts, return_tensors="pt", padding=True, truncation=True)
    with torch.no_grad():
        outputs = model(**inputs)
    embeddings = outputs.last_hidden_state.mean(dim=1).tolist()
    return embeddings

================
File: modules/embedding/similarity.py
================
from typing import List
from sklearn.metrics.pairwise import cosine_similarity

def calculate_similarity(embedding1: List[float], embedding2: List[float]) -> float:
    """
    Calculate cosine similarity between two embeddings.
    """
    return cosine_similarity([embedding1], [embedding2])[0][0]

================
File: modules/scoring/awards.py
================
from app.models.resume import AwardItem
from app.models.scoring_rules import SCORING_RULES
from typing import List, Optional

def calculate_awards_score(awards: List[AwardItem]) -> float:
    """
    Calculate the awards score based on contest prestige, prize, and description.
    """
    total_score = 0.0

    for award in awards:
        # Contest Prestige (30%)
        contest_score = calculate_contest_score(award.contest) * SCORING_RULES["awards"]["contest"]

        # Prize (25%)
        prize_score = calculate_prize_score(award.prize) * SCORING_RULES["awards"]["prize"]

        # Description (25%)
        description_score = calculate_description_score(award.description) * SCORING_RULES["awards"]["description"]

        # Role (10%)
        role_score = 10 if is_technical_role(award.role) else 0

        # Link (5%)
        link_score = 5 if award.link else 0

        # Time (5%)
        time_score = 5 if award.time else 0

        # Total Award Score
        total_score += contest_score + prize_score + description_score + role_score + link_score + time_score

    return total_score

def calculate_contest_score(contest: str) -> int:
    """
    Calculate the score based on the contest's prestige.
    """
    if "international" in contest.lower():
        return 10
    elif "national" in contest.lower():
        return 8
    elif "local" in contest.lower():
        return 5
    else:
        return 2

def calculate_prize_score(prize: str) -> int:
    """
    Calculate the score based on the prize level.
    """
    if "1st" in prize.lower() or "gold" in prize.lower():
        return 25
    elif "2nd" in prize.lower() or "silver" in prize.lower():
        return 20
    elif "3rd" in prize.lower() or "bronze" in prize.lower():
        return 15
    else:
        return 10

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity and impact.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

def is_technical_role(role: Optional[str]) -> bool:
    """
    Check if the role is relevant to technical fields.
    """
    if not role:
        return False
    technical_keywords = ["developer", "engineer", "data scientist", "analyst"]
    return any(keyword in role.lower() for keyword in technical_keywords)

================
File: modules/scoring/certifications.py
================
from app.models.resume import CertificationItem
from app.models.scoring_rules import SCORING_RULES
from typing import List, Optional

def calculate_certifications_score(certifications: List[CertificationItem]) -> float:
    """
    Calculate the certifications score based on name, organization, and link.
    """
    total_score = 0.0

    for cert in certifications:
        # Name (50%)
        name_score = calculate_name_score(cert.name) * SCORING_RULES["certifications"]["name"]

        # Organization (40%)
        org_score = calculate_org_score(cert.org) * SCORING_RULES["certifications"]["org"]

        # Link (10%)
        link_score = 10 if cert.link else 0

        # Total Certification Score
        total_score += name_score + org_score + link_score

    return total_score

def calculate_name_score(name: str) -> int:
    """
    Calculate the score based on the certification name's relevance.
    """
    if "aws" in name.lower() or "google" in name.lower() or "microsoft" in name.lower():
        return 50
    elif "data" in name.lower() or "cloud" in name.lower() or "ai" in name.lower():
        return 30
    else:
        return 10

def calculate_org_score(org: Optional[str]) -> int:
    """
    Calculate the score based on the organization's reputation.
    """
    if not org:
        return 0
    if "aws" in org.lower() or "google" in org.lower() or "microsoft" in org.lower():
        return 40
    elif "udemy" in org.lower() or "coursera" in org.lower():
        return 25
    else:
        return 10

================
File: modules/scoring/education.py
================
from app.models.resume import EducationItem
from app.models.scoring_rules import SCORING_RULES
from app.utils.json_lookup import get_university_score
from typing import List

def calculate_education_score(education: List[EducationItem]) -> float:
    """
    Calculate the education score based on school reputation, major relevance, and GPA.
    """
    total_score = 0.0

    for edu in education:
        # School Reputation (30%)
        school_score = get_university_score(edu.school) * SCORING_RULES["education"]["school"]

        # Major Relevance (30%)
        major_score = 30 if is_technical_major(edu.major) else 0

        # GPA (10%)
        gpa_score = 0
        if edu.gpa:
            if edu.gpa >= 3.2:
                gpa_score = 10
            elif 2.8 <= edu.gpa < 3.2:
                gpa_score = 5

        # Class Year (30%)
        class_score = calculate_class_score(edu.class_year)

        # Total Education Score
        total_score += school_score + major_score + gpa_score + class_score

    return total_score

def is_technical_major(major: str) -> bool:
    """
    Check if the major is relevant to technical fields.
    """
    technical_keywords = ["computer", "software", "data", "information", "cyber", "cloud", "web", "ai", "machine learning", "analytics"]
    return any(keyword in major.lower() for keyword in technical_keywords)

def calculate_class_score(class_year: str) -> int:
    """
    Calculate the score based on the class year.
    """
    if class_year.lower() == "senior":
        return 30
    elif class_year.lower() == "junior":
        return 20
    elif class_year.lower() == "sophomore":
        return 10
    else:
        return 10

================
File: modules/scoring/experience.py
================
from app.models.resume import ProfessionalExperienceItem
from app.models.scoring_rules import SCORING_RULES
from app.utils.json_lookup import get_company_score
from typing import List

def calculate_experience_score(experience: List[ProfessionalExperienceItem]) -> float:
    """
    Calculate the professional experience score based on company size, position relevance, and description.
    """
    total_score = 0.0

    for exp in experience:
        # Company Size (25%)
        company_score = get_company_score(exp.company) * (SCORING_RULES["professional_experience"]["company"] / 100)

        # Position Relevance (25%)
        position_score = 25 if is_technical_position(exp.position) else 0

        # Description (25%)
        description_score = calculate_description_score(exp.description) * (SCORING_RULES["professional_experience"]["description"] / 100)

        # Seniority (5%)
        seniority_score = 5 if exp.seniority.lower() in ["senior", "lead", "manager"] else 0

        # Duration (5%)
        duration_score = 5 if calculate_duration(exp.duration) >= 6 else 0

        # Total Experience Score
        total_score += company_score + position_score + description_score + seniority_score + duration_score

    return total_score

def is_technical_position(position: str) -> bool:
    """
    Check if the position is relevant to technical roles.
    """
    technical_keywords = ["software", "frontend", "backend", "web", "mobile", "full stack", "data", "machine learning", "ai", "devops"]
    return any(keyword in position.lower() for keyword in technical_keywords)

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity, impact, and use of tools.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

def calculate_duration(duration: str) -> int:
    """
    Calculate the duration in months.
    """
    try:
        # Handle different duration formats (e.g., "Jan 2020 - Dec 2021" or "2 years")
        if " - " in duration:
            # Format: "Jan 2020 - Dec 2021"
            start, end = duration.split(" - ")
            start_year = int(start.split()[-1])
            end_year = int(end.split()[-1])
            return (end_year - start_year) * 12
        elif "year" in duration:
            # Format: "2 years"
            years = int(duration.split()[0])
            return years * 12
        elif "month" in duration:
            # Format: "6 months"
            months = int(duration.split()[0])
            return months
        else:
            # Default to 0 if the format is unrecognized
            return 0
    except:
        # If parsing fails, return 0
        return 0

================
File: modules/scoring/projects.py
================
from typing import List, Optional
from app.models.resume import ProjectItem
from app.models.scoring_rules import SCORING_RULES

def calculate_projects_score(projects: List[ProjectItem]) -> float:
    """
    Calculate the projects score based on tech stack, GitHub link, and description.
    """
    total_score = 0.0

    for project in projects:
        # Tech Stack (25%)
        tech_score = 25 if is_technical_project(project.tech) else 0

        # GitHub Link (20%)
        link_score = 20 if project.link else 0

        # Description (40%)
        description_score = calculate_description_score(project.description)

        # Total Project Score
        total_score += tech_score + link_score + description_score

    return total_score

def is_technical_project(tech: Optional[str]) -> bool:
    """
    Check if the project uses relevant technologies.
    """
    if not tech:
        return False
    technical_keywords = ["python", "java", "sql", "tensorflow", "react", "docker"]
    return any(keyword in tech.lower() for keyword in technical_keywords)

def calculate_description_score(description: str) -> int:
    """
    Calculate the score based on the description's clarity, impact, and use of tools.
    """
    score = 0

    # Clarity and Focus (30%)
    if "developed" in description.lower() or "designed" in description.lower():
        score += 10

    # Achievements/Impact (30%)
    if "reduced" in description.lower() or "improved" in description.lower():
        score += 10

    # Use of Tools/Technologies (20%)
    if "python" in description.lower() or "sql" in description.lower():
        score += 10

    return score

================
File: modules/scoring/scorer.py
================
from app.models.resume import Resume
from app.models.scoring_rules import SCORING_RULES
from .education import calculate_education_score
from .experience import calculate_experience_score
from .projects import calculate_projects_score
from .awards import calculate_awards_score
from .certifications import calculate_certifications_score

def calculate_total_score(resume: Resume) -> float:
    """
    Calculate the total score for a resume by aggregating scores from all fields.
    """
    total_score = 0.0

    # Education Score
    education_score = calculate_education_score(resume.education)
    total_score += education_score * (SCORING_RULES["education"]["weight"] / 100)

    # Professional Experience Score
    experience_score = calculate_experience_score(resume.professional_experience)
    total_score += experience_score * (SCORING_RULES["professional_experience"]["weight"] / 100)

    # Projects Score
    projects_score = calculate_projects_score(resume.projects)
    total_score += projects_score * (SCORING_RULES["projects"]["weight"] / 100)

    # Awards Score
    awards_score = calculate_awards_score(resume.awards)
    total_score += awards_score * (SCORING_RULES["awards"]["weight"] / 100)

    # Certifications Score
    certifications_score = calculate_certifications_score(resume.certifications)
    total_score += certifications_score * (SCORING_RULES["certifications"]["weight"] / 100)

    return total_score

================
File: modules/summarization/evaluator.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import Resume

load_dotenv()

def evaluate_resume(resume: Resume) -> str:
    """
    Evaluate a resume using OpenAI's API.
    """
    # Initialize the OpenAI client
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Create the prompt from the resume data
    prompt = f"Please evaluate the following resume:\n{resume}"
    
    # Use the new chat completion endpoint
    response = client.chat.completions.create(
        model="gpt-4o",  
        messages=[
            {"role": "system", "content": "You are a professional resume evaluator. Provide a detailed evaluation of the resume."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=1000,
        temperature=0.7
    )
    
    # Extract the evaluation from the response
    evaluation = response.choices[0].message.content
    
    return evaluation

def format_resume_for_evaluation(resume: Resume) -> str:
    """
    Format the resume into a text string for evaluation.
    """
    evaluation = f"Name: {resume.name}\n"
    evaluation += f"Location: {resume.location}\n"
    evaluation += f"Email: {resume.email}\n"
    evaluation += f"LinkedIn: {resume.linkedin}\n"
    evaluation += f"Phone: {resume.phone}\n"
    evaluation += f"Intro: {resume.intro}\n"

    evaluation += "\nEducation:\n"
    for edu in resume.education:
        evaluation += f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}\n"

    evaluation += "\nProfessional Experience:\n"
    for exp in resume.professional_experience:
        evaluation += f"- {exp.company}, {exp.position}, {exp.duration}\n"
        evaluation += f"  {exp.description}\n"

    evaluation += "\nProjects:\n"
    for project in resume.projects:
        evaluation += f"- {project.name}, {project.tech}, {project.duration}\n"
        evaluation += f"  {project.description}\n"

    evaluation += "\nAwards:\n"
    for award in resume.awards:
        evaluation += f"- {award.contest}, {award.prize}, {award.time}\n"
        evaluation += f"  {award.description}\n"

    evaluation += "\nCertifications:\n"
    for cert in resume.certifications:
        evaluation += f"- {cert.name}, {cert.org}\n"

    evaluation += "\nSkills:\n"
    for skill in resume.skills:
        evaluation += f"- {skill.name}: {', '.join(skill.list)}\n"

    return evaluation

================
File: modules/summarization/summarizer.py
================
from openai import OpenAI
from dotenv import load_dotenv
import os
from app.models.resume import Resume

load_dotenv()

def summarize_resume(resume: Resume) -> str:
    """
    Summarize a resume using OpenAI's API.
    """
    # Initialize the OpenAI client
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    prompt = f"Please summarize the following resume:\n{resume}"
    
    response = client.chat.completions.create(
        model="gpt-4o",  
        messages=[
            {"role": "system", "content": "You are a professional resume reviewer. Summarize the key points of the resume concisely."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=500,
        temperature=0.7
    )
    
    # Extract the summary from the response
    summary = response.choices[0].message.content
    
    return summary

def format_resume_for_summary(resume: Resume) -> str:
    """
    Format the resume into a text string for summarization.
    """
    summary = f"Name: {resume.name}\n"
    summary += f"Location: {resume.location}\n"
    summary += f"Email: {resume.email}\n"
    summary += f"LinkedIn: {resume.linkedin}\n"
    summary += f"Phone: {resume.phone}\n"
    summary += f"Intro: {resume.intro}\n"

    summary += "\nEducation:\n"
    for edu in resume.education:
        summary += f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}\n"

    summary += "\nProfessional Experience:\n"
    for exp in resume.professional_experience:
        summary += f"- {exp.company}, {exp.position}, {exp.duration}\n"
        summary += f"  {exp.description}\n"

    summary += "\nProjects:\n"
    for project in resume.projects:
        summary += f"- {project.name}, {project.tech}, {project.duration}\n"
        summary += f"  {project.description}\n"

    summary += "\nAwards:\n"
    for award in resume.awards:
        summary += f"- {award.contest}, {award.prize}, {award.time}\n"
        summary += f"  {award.description}\n"

    summary += "\nCertifications:\n"
    for cert in resume.certifications:
        summary += f"- {cert.name}, {cert.org}\n"

    summary += "\nSkills:\n"
    for skill in resume.skills:
        summary += f"- {skill.name}: {', '.join(skill.list)}\n"

    return summary

================
File: tests/tempCodeRunnerFile.py
================
import pytest
from app.models.resume import ProfessionalExperienceItem
from app.modules.scoring.experience import calculate_experience_score

def test_calculate_experience_score():
    experience = [
        ProfessionalExperienceItem(company="FPT Software", location="Hanoi", position="Software Engineer", seniority="Junior", duration="2 years", description="Developed web applications using Python and Django.")
    ]
    score = calculate_experience_score(experience)
    assert score == 65  # FPT Software (20) + Software Engineer (25) + Description (20)

================
File: tests/test_awards.py
================
from app.models.resume import AwardItem
from app.modules.scoring.awards import calculate_awards_score

def test_calculate_awards_score():
    awards = [
        AwardItem(contest="International Coding Competition", prize="1st Place", description="Developed an AI model using Python.", role="Lead Developer", link="https://example.com", time="2023"),
        AwardItem(contest="Local Hackathon", prize="2nd Place", description="Built a web app using React.", role="Frontend Developer", link=None, time="2022")
    ]
    score = calculate_awards_score(awards)

    # Output the awards score
    print("=== Awards Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_awards_score()

================
File: tests/test_certification.py
================
from app.models.resume import CertificationItem
from app.modules.scoring.certifications import calculate_certifications_score

def test_calculate_certifications_score():
    certifications = [
        CertificationItem(name="AWS Certified Solutions Architect", org="Amazon Web Services", link="https://example.com"),
        CertificationItem(name="Google Data Engineer", org="Google", link=None)
    ]
    score = calculate_certifications_score(certifications)

    # Output the certifications score
    print("=== Certifications Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_certifications_score()

================
File: tests/test_cv_evaluation.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.modules.summarization.evaluator import evaluate_resume

def test_cv_evaluation():
    # Path to the CV file (PDF or DOCX)
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  # Replace with the actual path to your CV file

    # Extract structured data from the CV
    resume = extract_resume(cv_file)

    # Evaluate the CV using an LLM
    evaluation = evaluate_resume(resume)

    # Print the evaluation
    print("=== CV Evaluation ===")
    print(evaluation)

# Run the test
test_cv_evaluation()

================
File: tests/test_cv_summarization.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.modules.summarization.summarizer import summarize_resume

def test_cv_summarization():
    # Path to the CV file (PDF or DOCX)
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  # Replace with the actual path to your CV file

    # Extract structured data from the CV
    resume = extract_resume(cv_file)

    # Summarize the CV using an LLM
    summary = summarize_resume(resume)

    # Print the summary
    print("=== CV Summary ===")
    print(summary)

# Run the test
test_cv_summarization()

================
File: tests/test_document_extraction.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.models.resume import Resume

def test_document_extraction():
    # Path to the CV file (PDF or DOCX)
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  # Replace with the actual path to your CV file

    # Extract structured data from the CV using LLM
    resume = extract_resume(cv_file)

    # Test that the returned value is a dictionary
    assert isinstance(resume, Resume)
    
    # Test that required fields exist and have expected types
    assert isinstance(resume.name, str)
    assert isinstance(resume.email, str)
    assert isinstance(resume.phone, str)
    assert isinstance(resume.education, list)
    assert isinstance(resume.professional_experience, list)
    assert isinstance(resume.skills, list)
    
    # Test that required fields are not empty or default values
    assert resume.name != ""
    assert resume.email != ""
    assert resume.phone != ""

    # Print the extracted resume data
    print("=== Extracted Resume ===")
    print(f"Name: {resume.name}")
    print(f"Location: {resume.location}")
    print(f"Email: {resume.email}")
    print(f"LinkedIn: {resume.linkedin}")
    print(f"Phone: {resume.phone}")
    print(f"Intro: {resume.intro}")
    print("\nEducation:")
    for edu in resume.education:
        print(f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}")
    print("\nProfessional Experience:")
    for exp in resume.professional_experience:
        print(f"- {exp.company}, {exp.position}, {exp.duration}")
        print(f"  {exp.description}")
    print("\nProjects:")
    for project in resume.projects:
        print(f"- {project.name}, {project.tech}, {project.duration}")
        print(f"  {project.description}")
    print("\nAwards:")
    for award in resume.awards:
        print(f"- {award.contest}, {award.prize}, {award.time}")
        print(f"  {award.description}")
    print("\nCertifications:")
    for cert in resume.certifications:
        print(f"- {cert.name}, {cert.org}")
    print("\nSkills:")
    for skill in resume.skills:
        print(f"- {skill.name}: {', '.join(skill.list)}")

# Run the test
test_document_extraction()

================
File: tests/test_education.py
================
from app.models.resume import EducationItem
from app.modules.scoring.education import calculate_education_score

def test_calculate_education_score():
    education = [
        EducationItem(school="HUST", class_year="Senior", major="Computer Science", gpa=3.5)
    ]
    score = calculate_education_score(education)

    # Output the education score
    print("=== Education Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_education_score()

================
File: tests/test_embedding.py
================
from app.modules.embedding.embedder import get_embeddings

def test_get_embeddings():
    texts = ["Python developer", "Data scientist"]
    print("Processing")
    embeddings = get_embeddings(texts)

    # Output the embeddings
    print("=== Embeddings ===")
    for i, embedding in enumerate(embeddings):
        print(f"Text: {texts[i]}")
        print(f"Embedding: {embedding[:5]}...")  # Print first 5 values for brevity

# Run the test
test_get_embeddings()

================
File: tests/test_experience.py
================
from app.models.resume import ProfessionalExperienceItem
from app.modules.scoring.experience import calculate_experience_score

def test_calculate_experience_score():
    experience = [
        ProfessionalExperienceItem(company="FPT Software", location="Hanoi", position="Software Engineer", seniority="Junior", duration="2 years", description="Developed web applications using Python and Django.")
    ]
    score = calculate_experience_score(experience)

    # Output the experience score
    print("=== Experience Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_experience_score()

================
File: tests/test_scoring.py
================
from app.models.resume import Resume, EducationItem, ProfessionalExperienceItem, ProjectItem
from app.modules.scoring.scorer import calculate_total_score

def test_calculate_total_score():
    resume = Resume(
        name="John Doe",
        location="Hanoi",
        social=[],
        email="johndoe@example.com",
        phone="1234567890",
        intro="A passionate software engineer...",
        education=[
            EducationItem(school="HUST", class_year="Senior", major="Computer Science", gpa=3.5)
        ],
        professional_experience=[
            ProfessionalExperienceItem(
                company="FPT Software",
                location="Hanoi",
                position="Software Engineer",
                seniority="Junior",
                duration="Jan 2020 - Dec 2021",  # Valid duration format
                description="Developed web applications using Python and Django."
            )
        ],
        projects=[
            ProjectItem(
                name="E-commerce Website",
                tech="Python, Django, React",
                description="Developed a full-stack e-commerce website."
            )
        ],
        awards=[],
        certifications=[],
        skills=[]
    )
    score = calculate_total_score(resume)

    # Output the total score
    print("=== Total Resume Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_total_score()

================
File: utils/bias_check.py
================
def check_bias(text: str) -> bool:
    """
    Check for bias in the text (e.g., gender, ethnicity).
    """
    bias_keywords = ["male", "female", "asian", "black", "white"]
    return any(keyword in text.lower() for keyword in bias_keywords)

================
File: utils/json_lookup.py
================
import json
from pathlib import Path

def load_json_data(file_name: str) -> dict:
    """
    Load JSON data from the data folder.
    """
    file_path = Path(__file__).parent.parent / "data" / file_name
    with open(file_path, "r") as file:
        return json.load(file)

def get_university_score(university: str) -> int:
    """
    Get the score for a university based on its reputation.
    """
    universities = load_json_data("universities.json")
    return universities.get(university, 10)

def get_company_score(company: str) -> int:
    """
    Get the score for a company based on its size.
    """
    companies = load_json_data("companies.json")
    return companies.get(company, 10)

================
File: utils/normalization.py
================
import unicodedata

def normalize_text(text: str) -> str:
    """
    Normalize text by removing diacritics and converting to lowercase.
    """
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("ascii")
    return text.lower()

================
File: app/test_api.py
================
import requests
import json
from pathlib import Path
import logging
import mimetypes
import time
from typing import Optional, Dict, Any
import sys

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('test_api.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class CVAPITester:
    def __init__(self, api_url: str = "http://localhost:8000/api/evaluate-cv"):
        self.api_url = api_url
        self.session = requests.Session()

    def validate_file(self, file_path: Path) -> bool:
        """
        Validate the CV file before sending.
        """
        try:
            if not file_path.exists():
                logger.error(f"File not found: {file_path}")
                return False

            if not file_path.suffix.lower() == '.pdf':
                logger.error(f"Invalid file format. Expected PDF, got: {file_path.suffix}")
                return False

            # Check file size (max 10MB)
            max_size = 10 * 1024 * 1024  # 10MB in bytes
            if file_path.stat().st_size > max_size:
                logger.error(f"File too large: {file_path.stat().st_size / 1024 / 1024:.2f}MB (max 10MB)")
                return False

            # Check if file is readable
            try:
                with open(file_path, 'rb') as f:
                    f.read(1024)  # Try reading first 1KB
            except Exception as e:
                logger.error(f"File not readable: {str(e)}")
                return False

            return True

        except Exception as e:
            logger.error(f"File validation error: {str(e)}")
            return False

    def send_cv(self, file_path: Path, retries: int = 3, delay: int = 2) -> Optional[Dict[str, Any]]:
        """
        Send CV to API with retries and error handling.
        """
        if not self.validate_file(file_path):
            return None

        for attempt in range(retries):
            try:
                with open(file_path, 'rb') as pdf_file:
                    files = {
                        "file": (file_path.name, pdf_file, "application/pdf")
                    }
                    
                    logger.info(f"Sending request to {self.api_url}")
                    logger.info(f"Attempt {attempt + 1} of {retries}")
                    
                    response = self.session.post(self.api_url, files=files)
                    
                    # Log response info
                    logger.info(f"Status Code: {response.status_code}")
                    logger.info(f"Response Headers: {dict(response.headers)}")
                    
                    if response.ok:
                        result = response.json()
                        logger.info("Request successful!")
                        return result
                    else:
                        logger.error(f"Request failed: {response.status_code}")
                        logger.error(f"Error response: {response.text}")
                        
                        if response.status_code == 413:
                            logger.error("File too large for server")
                            break  # Don't retry if file is too large
                        elif attempt < retries - 1:
                            time.sleep(delay)  # Wait before retrying
                        else:
                            logger.error("Max retries reached")
                            
            except requests.exceptions.ConnectionError:
                logger.error(f"Connection error (attempt {attempt + 1})")
                if attempt < retries - 1:
                    time.sleep(delay)
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                if attempt < retries - 1:
                    time.sleep(delay)
                    
        return None

    def save_result(self, result: Dict[str, Any], output_dir: Path) -> None:
        """
        Save API response to JSON file.
        """
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            output_file = output_dir / f"cv_result_{timestamp}.json"
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
                
            logger.info(f"Results saved to: {output_file}")
            
        except Exception as e:
            logger.error(f"Error saving results: {str(e)}")

def main():
    """
    Main function to test the CV evaluation API.
    """
    # Configuration
    api_url = "http://localhost:8000/api/evaluate-cv"
    output_dir = Path("results")
    
    if len(sys.argv) < 2:
        logger.error("Please provide the path to your CV file as an argument")
        logger.info("Usage: python test_api.py path/to/your/cv.pdf")
        sys.exit(1)
        
    cv_path = Path(sys.argv[1])
    
    try:
        # Initialize tester
        tester = CVAPITester(api_url)
        
        # Send CV and get results
        logger.info(f"Processing CV: {cv_path}")
        result = tester.send_cv(cv_path)
        
        if result:
            # Save results
            tester.save_result(result, output_dir)
            
            # Print key information
            print("\nCV Processing Results:")
            print("-" * 50)
            if "scores" in result:
                print("\nScores:")
                for category, score in result["scores"].items():
                    print(f"{category}: {score}")
                    
            if "ai_summary" in result:
                print("\nSummary:")
                print(result["ai_summary"])
            
            if "ai_evaluation" in result:
                print("\nEvaluation:")
                print(result["ai_evaluation"])
        else:
            logger.error("Failed to process CV")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()

================
File: app/tests/tempCodeRunnerFile.py
================
import pytest
from app.models.resume import ProfessionalExperienceItem
from app.modules.scoring.experience import calculate_experience_score

def test_calculate_experience_score():
    experience = [
        ProfessionalExperienceItem(company="FPT Software", location="Hanoi", position="Software Engineer", seniority="Junior", duration="2 years", description="Developed web applications using Python and Django.")
    ]
    score = calculate_experience_score(experience)
    assert score == 65  # FPT Software (20) + Software Engineer (25) + Description (20)

================
File: app/tests/test_awards.py
================
from app.models.resume import AwardItem
from app.modules.scoring.awards import calculate_awards_score

def test_calculate_awards_score():
    awards = [
        AwardItem(contest="International Coding Competition", prize="1st Place", description="Developed an AI model using Python.", role="Lead Developer", link="https://example.com", time="2023"),
        AwardItem(contest="Local Hackathon", prize="2nd Place", description="Built a web app using React.", role="Frontend Developer", link=None, time="2022")
    ]
    score = calculate_awards_score(awards)

    # Output the awards score
    print("=== Awards Score ===")
    print(f"Score: {score}")

test_calculate_awards_score()

================
File: app/tests/test_certification.py
================
from app.models.resume import CertificationItem
from app.modules.scoring.certifications import calculate_certifications_score

def test_calculate_certifications_score():
    certifications = [
        CertificationItem(name="AWS Certified Solutions Architect", org="Amazon Web Services", link="https://example.com"),
        CertificationItem(name="Google Data Engineer", org="Google", link=None)
    ]
    score = calculate_certifications_score(certifications)

    # Output the certifications score
    print("=== Certifications Score ===")
    print(f"Score: {score}")

# Run the test
test_calculate_certifications_score()

================
File: app/tests/test_cv_evaluation.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.modules.summarization.evaluator import evaluate_resume

def test_cv_evaluation():
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  

    # Extract structured data from the CV
    resume = extract_resume(cv_file)

    # Evaluate the CV using an LLM
    evaluation = evaluate_resume(resume)

    # Print the evaluation
    print("=== CV Evaluation ===")
    print(evaluation)

test_cv_evaluation()

================
File: app/tests/test_cv_summarization.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.modules.summarization.summarizer import summarize_resume

def test_cv_summarization():
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  

    resume = extract_resume(cv_file)

    summary = summarize_resume(resume)

    # Print the summary
    print("=== CV Summary ===")
    print(summary)

test_cv_summarization()

================
File: app/tests/test_document_extraction.py
================
from app.modules.document_extraction.extractor import extract_resume
from app.models.resume import Resume

def test_document_extraction():
    cv_file = "D:\Downloads\LeMinhLocCV.pdf"  

    resume = extract_resume(cv_file)

    # Test that the returned value is a dictionary
    assert isinstance(resume, Resume)
    
    # Test that required fields exist and have expected types
    assert isinstance(resume.name, str)
    assert isinstance(resume.email, str)
    assert isinstance(resume.phone, str)
    assert isinstance(resume.education, list)
    assert isinstance(resume.professional_experience, list)
    assert isinstance(resume.skills, list)
    
    # Test that required fields are not empty or default values
    assert resume.name != ""
    assert resume.email != ""
    assert resume.phone != ""

    # Print the extracted resume data
    print("=== Extracted Resume ===")
    print(f"Name: {resume.name}")
    print(f"Location: {resume.location}")
    print(f"Email: {resume.email}")
    print(f"LinkedIn: {resume.linkedin}")
    print(f"Phone: {resume.phone}")
    print(f"Intro: {resume.intro}")
    print("\nEducation:")
    for edu in resume.education:
        print(f"- {edu.school}, {edu.major}, {edu.class_year}, GPA: {edu.gpa}")
    print("\nProfessional Experience:")
    for exp in resume.professional_experience:
        print(f"- {exp.company}, {exp.position}, {exp.duration}")
        print(f"  {exp.description}")
    print("\nProjects:")
    for project in resume.projects:
        print(f"- {project.name}, {project.tech}, {project.duration}")
        print(f"  {project.description}")
    print("\nAwards:")
    for award in resume.awards:
        print(f"- {award.contest}, {award.prize}, {award.time}")
        print(f"  {award.description}")
    print("\nCertifications:")
    for cert in resume.certifications:
        print(f"- {cert.name}, {cert.org}")
    print("\nSkills:")
    for skill in resume.skills:
        print(f"- {skill.name}: {', '.join(skill.list)}")

test_document_extraction()

================
File: app/tests/test_education.py
================
from app.models.resume import EducationItem
from app.modules.scoring.education import calculate_education_score

def test_calculate_education_score():
    education = [
        EducationItem(school="HUST", class_year="Senior", major="Computer Science", gpa=3.5)
    ]
    score = calculate_education_score(education)

    # Output the education score
    print("=== Education Score ===")
    print(f"Score: {score}")

test_calculate_education_score()

================
File: app/tests/test_embedding.py
================
from app.modules.embedding.embedder import get_embeddings

def test_get_embeddings():
    texts = ["Python developer", "Data scientist"]
    print("Processing")
    embeddings = get_embeddings(texts)

    # Output the embeddings
    print("=== Embeddings ===")
    for i, embedding in enumerate(embeddings):
        print(f"Text: {texts[i]}")
        print(f"Embedding: {embedding[:5]}...")  # Print first 5 values for brevity

test_get_embeddings()

================
File: app/tests/test_experience.py
================
from app.models.resume import ProfessionalExperienceItem
from app.modules.scoring.experience import calculate_experience_score

def test_calculate_experience_score():
    experience = [
        ProfessionalExperienceItem(company="FPT Software", location="Hanoi", position="Software Engineer", seniority="Junior", duration="2 years", description="Developed web applications using Python and Django.")
    ]
    score = calculate_experience_score(experience)

    # Output the experience score
    print("=== Experience Score ===")
    print(f"Score: {score}")

test_calculate_experience_score()

================
File: app/tests/test_scoring.py
================
from app.models.resume import Resume, EducationItem, ProfessionalExperienceItem, ProjectItem
from app.modules.scoring.scorer import calculate_total_score

def test_calculate_total_score():
    resume = Resume(
        name="John Doe",
        location="Hanoi",
        social=[],
        email="johndoe@example.com",
        phone="1234567890",
        intro="A passionate software engineer...",
        education=[
            EducationItem(school="HUST", class_year="Senior", major="Computer Science", gpa=3.5)
        ],
        professional_experience=[
            ProfessionalExperienceItem(
                company="FPT Software",
                location="Hanoi",
                position="Software Engineer",
                seniority="Junior",
                duration="Jan 2020 - Dec 2021",  # Valid duration format
                description="Developed web applications using Python and Django."
            )
        ],
        projects=[
            ProjectItem(
                name="E-commerce Website",
                tech="Python, Django, React",
                description="Developed a full-stack e-commerce website."
            )
        ],
        awards=[],
        certifications=[],
        skills=[]
    )
    score = calculate_total_score(resume)

    # Output the total score
    print("=== Total Resume Score ===")
    print(f"Score: {score}")

test_calculate_total_score()

================
File: app/utils/bias_check.py
================
def check_bias(text: str) -> bool:
    """
    Check for bias in the text (e.g., gender, ethnicity).
    """
    bias_keywords = ["male", "female", "asian", "black", "white"]
    return any(keyword in text.lower() for keyword in bias_keywords)

================
File: app/utils/json_lookup.py
================
import json
from pathlib import Path

def load_json_data(file_name: str) -> dict:
    """
    Load JSON data from the data folder.
    """
    file_path = Path(__file__).parent.parent / "data" / file_name
    with open(file_path, "r") as file:
        return json.load(file)

def get_university_score(university: str) -> int:
    """
    Get the score for a university based on its reputation.
    """
    universities = load_json_data("universities.json")
    return universities.get(university, 10)

def get_company_score(company: str) -> int:
    """
    Get the score for a company based on its size.
    """
    companies = load_json_data("companies.json")
    return companies.get(company, 10)

================
File: app/utils/normalization.py
================
import unicodedata
from typing import Dict

# Dictionary for Vietnamese university synonyms
UNIVERSITY_SYNONYMS: Dict[str, str] = {
    "Đại học Bách Khoa Hà Nội": "HUST",
    "Đại học Bách Khoa": "HUST",
    "Bách Khoa Hà Nội": "HUST",
    "HUST": "HUST",
    "RMIT University Vietnam": "RMIT",
    "RMIT": "RMIT",
    "VinUniversity": "VinUni",
    "VinUni": "VinUni",
}

def normalize_text(text: str) -> str:
    """
    Normalize text by removing diacritics and converting to lowercase.
    """
    text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("ascii")
    return text.lower()

def normalize_university_name(university: str) -> str:
    """
    Normalize university names using a synonym dictionary.
    """
    # Remove diacritics and convert to lowercase
    normalized_name = normalize_text(university)
    
    # Check if the normalized name matches any synonym
    for key, value in UNIVERSITY_SYNONYMS.items():
        if normalize_text(key) == normalized_name:
            return value
    
    # If no match, return the original normalized name
    return normalized_name

================
File: batch_process.py
================
import os
import json
from datetime import datetime
from pathlib import Path
import requests
from typing import Dict, List, Any
import logging
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cv_processing.log'),
        logging.StreamHandler()
    ]
)

class BatchCVProcessor:
    def __init__(self, cv_directory: str, output_directory: str, api_url: str = "http://localhost:8000/api/evaluate-cv"):
        self.cv_directory = Path(cv_directory)
        self.output_directory = Path(output_directory)
        self.api_url = api_url
        self.openai_client = OpenAI()
        self.results: Dict[str, Any] = {
            "processing_date": datetime.now().isoformat(),
            "total_cvs": 0,
            "successful": 0,
            "failed": 0,
            "evaluations": []
        }
        
        # Create output directory if it doesn't exist
        self.output_directory.mkdir(parents=True, exist_ok=True)

    def get_llm_analysis(self, cv_data: Dict[str, Any], scores: Dict[str, Any]) -> Dict[str, Any]:
        """Get detailed analysis from LLM."""
        try:
            # Prepare the prompt
            prompt = f"""
            Analyze this CV and provide detailed professional feedback. Here's the data:

            CV Data:
            {json.dumps(cv_data, indent=2)}

            Scores:
            {json.dumps(scores, indent=2)}

            Please provide:
            1. A detailed analysis of the candidate's profile
            2. Specific reasons for the pass/fail decision
            3. Key strengths with concrete examples
            4. Areas for improvement with actionable advice
            5. Career trajectory analysis and potential
            6. Technical skill assessment
            7. Cultural fit indicators
            8. Risk factors if any

            Format your response as a JSON object with these keys. Be specific and professional.
            """

            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert technical recruiter with deep knowledge of the software industry."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )

            # Parse the response
            try:
                llm_analysis = json.loads(response.choices[0].message.content)
            except json.JSONDecodeError:
                # If JSON parsing fails, structure the raw response
                llm_analysis = {
                    "detailed_analysis": response.choices[0].message.content,
                    "error": "Failed to parse structured response"
                }

            return llm_analysis

        except Exception as e:
            logging.error(f"LLM analysis error: {str(e)}")
            return {
                "error": f"Failed to get LLM analysis: {str(e)}",
                "detailed_analysis": "Analysis unavailable"
            }

    def analyze_scores(self, scores: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze scores and provide detailed reasoning."""
        analysis = {
            "status": scores.get("status", "Unknown"),
            "total_score": scores.get("total_score", 0),
            "strengths": [],
            "weaknesses": [],
            "recommendations": []
        }

        # Define score thresholds
        thresholds = {
            "education": {"high": 12, "low": 8},  # out of 15
            "experience": {"high": 20, "low": 15},  # out of 25
            "projects": {"high": 15, "low": 10},  # out of 20
            "awards": {"high": 12, "low": 8},  # out of 15
            "certifications": {"high": 4, "low": 2},  # out of 5
        }

        # Analyze each component
        for category, score in scores.items():
            if category in thresholds:
                if score >= thresholds[category]["high"]:
                    analysis["strengths"].append(f"Strong {category} background (Score: {score})")
                elif score <= thresholds[category]["low"]:
                    analysis["weaknesses"].append(f"Weak {category} background (Score: {score})")
                    if category == "education":
                        analysis["recommendations"].append("Consider candidates from top-tier universities or with more relevant majors")
                    elif category == "experience":
                        analysis["recommendations"].append("Look for candidates with more relevant technical experience")
                    elif category == "projects":
                        analysis["recommendations"].append("Seek candidates with more substantial technical projects")
                    elif category == "certifications":
                        analysis["recommendations"].append("Encourage obtaining relevant technical certifications")

        # Overall analysis
        if analysis["status"] == "Pass":
            analysis["reason"] = "Candidate meets the minimum requirements with a well-rounded profile"
            if len(analysis["weaknesses"]) > 0:
                analysis["reason"] += " despite some areas for improvement"
        else:
            analysis["reason"] = "Candidate does not meet minimum requirements due to: " + ", ".join(analysis["weaknesses"])

        return analysis

    def process_cv(self, cv_path: Path) -> Dict[str, Any]:
        """Process a single CV file."""
        try:
            logging.info(f"Processing CV: {cv_path.name}")
            
            # Validate file
            if not cv_path.exists():
                raise FileNotFoundError(f"File not found: {cv_path}")
            
            if not cv_path.suffix.lower() == '.pdf':
                raise ValueError(f"File must be PDF format: {cv_path}")

            # Send CV to API
            with open(cv_path, 'rb') as pdf_file:
                files = {
                    "file": (cv_path.name, pdf_file, "application/pdf")
                }
                response = requests.post(self.api_url, files=files)
                
                if response.ok:
                    result = response.json()
                    
                    # Add rule-based analysis
                    if "scores" in result:
                        result["rule_based_analysis"] = self.analyze_scores(result["scores"])
                    
                    # Add LLM analysis
                    if "cv_data" in result and "scores" in result:
                        result["llm_analysis"] = self.get_llm_analysis(result["cv_data"], result["scores"])
                    
                    result["file_name"] = cv_path.name
                    result["processing_status"] = "success"
                    return result
                else:
                    error_msg = f"API Error: {response.status_code} - {response.text}"
                    logging.error(error_msg)
                    return {
                        "file_name": cv_path.name,
                        "processing_status": "failed",
                        "error": error_msg
                    }

        except Exception as e:
            error_msg = f"Processing Error: {str(e)}"
            logging.error(error_msg)
            return {
                "file_name": cv_path.name,
                "processing_status": "failed",
                "error": error_msg
            }

    def process_all_cvs(self) -> None:
        """Process all CVs in the directory."""
        cv_files = list(self.cv_directory.glob("*.pdf"))
        self.results["total_cvs"] = len(cv_files)
        
        logging.info(f"Found {len(cv_files)} CV files to process")
        
        for cv_path in cv_files:
            result = self.process_cv(cv_path)
            self.results["evaluations"].append(result)
            
            if result["processing_status"] == "success":
                self.results["successful"] += 1
            else:
                self.results["failed"] += 1

    def generate_summary_statistics(self) -> Dict[str, Any]:
        """Generate detailed summary statistics."""
        successful_evals = [eval for eval in self.results["evaluations"] 
                          if eval.get("processing_status") == "success"]
        
        summary = {
            "processing_date": self.results["processing_date"],
            "total_cvs": self.results["total_cvs"],
            "successful": self.results["successful"],
            "failed": self.results["failed"],
            "pass_rate": sum(1 for eval in successful_evals 
                           if eval.get("scores", {}).get("status") == "Pass") / len(successful_evals)
            if successful_evals else 0,
            "category_statistics": {
                "education": {"avg": 0, "min": 0, "max": 0},
                "experience": {"avg": 0, "min": 0, "max": 0},
                "projects": {"avg": 0, "min": 0, "max": 0},
                "awards": {"avg": 0, "min": 0, "max": 0},
                "certifications": {"avg": 0, "min": 0, "max": 0}
            },
            "common_strengths": [],
            "common_weaknesses": [],
            "common_recommendations": [],
            "llm_insights": self.generate_llm_insights(successful_evals)
        }

        # Calculate category statistics
        if successful_evals:
            for category in summary["category_statistics"].keys():
                scores = [eval.get("scores", {}).get(category, 0) for eval in successful_evals]
                if scores:
                    summary["category_statistics"][category] = {
                        "avg": sum(scores) / len(scores),
                        "min": min(scores),
                        "max": max(scores)
                    }

        # Aggregate common insights
        all_strengths = []
        all_weaknesses = []
        all_recommendations = []
        
        for eval in successful_evals:
            analysis = eval.get("rule_based_analysis", {})
            all_strengths.extend(analysis.get("strengths", []))
            all_weaknesses.extend(analysis.get("weaknesses", []))
            all_recommendations.extend(analysis.get("recommendations", []))

        # Get most common insights
        summary["common_strengths"] = self._get_most_common(all_strengths, 5)
        summary["common_weaknesses"] = self._get_most_common(all_weaknesses, 5)
        summary["common_recommendations"] = self._get_most_common(all_recommendations, 5)

        return summary

    def generate_llm_insights(self, evaluations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate overall insights using LLM."""
        try:
            # Prepare the data for LLM analysis
            prompt = f"""
            Analyze these {len(evaluations)} CV evaluations and provide high-level insights:

            Evaluation Data:
            {json.dumps([{
                'scores': eval.get('scores', {}),
                'llm_analysis': eval.get('llm_analysis', {})
            } for eval in evaluations], indent=2)}

            Please provide:
1. Overall trends in the candidate pool
2. Common skill gaps and areas for improvement
3. Recommendations for improving the candidate pipeline
4. Notable patterns in successful vs unsuccessful candidates
5. Suggestions for recruitment strategy

            Format your response as a JSON object with these keys. Be specific and data-driven.
            """

            response = self.openai_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are an expert in technical recruitment and workforce analysis."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7
            )

            try:
                return json.loads(response.choices[0].message.content)
            except json.JSONDecodeError:
                return {
                    "overall_insights": response.choices[0].message.content,
                    "error": "Failed to parse structured response"
                }

        except Exception as e:
            logging.error(f"LLM insights error: {str(e)}")
            return {
                "error": f"Failed to generate LLM insights: {str(e)}",
                "overall_insights": "Insights unavailable"
            }

    def _get_most_common(self, items: List[str], n: int) -> List[str]:
        """Get n most common items from a list."""
        if not items:
            return []
        from collections import Counter
        return [item for item, count in Counter(items).most_common(n)]

    def save_results(self) -> None:
        """Save processing results to JSON file."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save detailed results
        output_file = self.output_directory / f"cv_evaluation_results_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        logging.info(f"Results saved to: {output_file}")
        
        # Save summary with statistics
        summary = self.generate_summary_statistics()
        summary_file = self.output_directory / f"cv_evaluation_summary_{timestamp}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        logging.info(f"Summary saved to: {summary_file}")

def main():
    # Configuration
    cv_directory = "./cvs"  # Directory containing CVs
    output_directory = "./results"  # Directory for results
    
    try:
        # Create processor instance
        processor = BatchCVProcessor(cv_directory, output_directory)
        
        # Process all CVs
        processor.process_all_cvs()
        
        # Save results
        processor.save_results()
        
        logging.info("Processing completed successfully")
        
    except Exception as e:
        logging.error(f"Main process error: {str(e)}")

if __name__ == "__main__":
    main()

================
File: evaluation_results.json
================
{
    "cv_data": {
        "name": "John Doe",
        "location": "Hanoi",
        "social": [],
        "email": "johndoe@example.com",
        "linkedin": null,
        "phone": "1234567890",
        "intro": "A passionate software engineer...",
        "education": [
            {
                "school": "HUST",
                "class_year": "Senior",
                "major": "Computer Science",
                "minor": null,
                "gpa": 3.5
            }
        ],
        "professional_experience": [
            {
                "company": "FPT Software",
                "location": "Hanoi",
                "position": "Software Engineer",
                "seniority": "Junior",
                "duration": "2 years",
                "description": "Developed web applications using Python and Django."
            }
        ],
        "projects": [
            {
                "name": "E-commerce Website",
                "link": null,
                "tech": "Python, Django, React",
                "duration": null,
                "description": "Developed a full-stack e-commerce website."
            }
        ],
        "awards": [],
        "certifications": [],
        "skills": []
    },
    "scores": {
        "education": 85,
        "experience": 65,
        "projects": 70,
        "awards": 0,
        "certifications": 0,
        "total_score": 72.5,
        "status": "Pass"
    },
    "ai_summarization": "John Doe is a Senior at HUST studying Computer Science with a GPA of 3.5. He has 2 years of experience as a Software Engineer at FPT Software, where he developed web applications using Python and Django. He also worked on a full-stack e-commerce website using Python, Django, and React.",
    "ai_evaluation": "John Doe has a strong educational background and relevant professional experience. His technical skills in Python, Django, and React are impressive. However, he lacks certifications and awards, which could strengthen his profile further."
}

================
File: README.md
================
---

## Installation

### Prerequisites
- Python 3.8 or higher
- OpenAI API key (for GPT-4 integration)
- Required Python libraries (see `requirements.txt`)

### Steps
1. Clone the repository:
   ```bash
   git clone https://github.com/your-username/ai-cv-screening.git
   cd ai-cv-screening
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up environment variables:
   - Create a `.env` file in the root directory.
   - Add your OpenAI API key:
     ```plaintext
     OPENAI_API_KEY=your_openai_api_key
     ```

---

## Usage

### Extract CV Data
To extract structured JSON data from a CV:
```python
from main import get_cv_json_data

cv_file_path = "path/to/your/cv.pdf"
cv_data = get_cv_json_data(cv_file_path)
print(cv_data)
```

### Evaluate CV
To evaluate a CV and get detailed evaluation information:
```python
from main import evaluate_cv

cv_file_path = "path/to/your/cv.pdf"
evaluation_info = evaluate_cv(cv_file_path)
print(evaluation_info)
```

### Export to JSON
To export evaluation results to a JSON file:
```python
from main import evaluate_cv, export_to_json

cv_file_path = "path/to/your/cv.pdf"
evaluation_info = evaluate_cv(cv_file_path)
export_to_json(evaluation_info, "evaluation_results.json")
```

### Run the API server
To run the API server:
```python
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000
```
---


## Configuration

### JSON Lookup Files
- **`data/universities.json`**: Contains university rankings and reputation scores.
- **`data/companies.json`**: Contains company size categories and reputation scores.
- **`data/technical_terms.json`**: Contains technical keywords for semantic analysis.

### Scoring Rules
- Scoring rules and weights are defined in `models/scoring_rules.py`.
- You can modify the weights to align with your organization's hiring criteria.

---

## Testing

To run unit tests:
- First add your pdf cv into the root folder
- Then change the path of the cv in test_api.py
- After that run 
```bash
cd app
python test_api.py
```

================
File: requirements.txt
================
PyPDF2>=3.0.0           
python-docx>=0.8.11    
openai>=0.27.0          

pydantic>=1.10.0        

transformers>=4.25.0    
torch>=1.13.0           
scikit-learn>=1.0.0    

python-dotenv>=0.21.0   

pytest>=7.0.0          
pytest-cov>=3.0.0       

requests>=2.28.0       
numpy>=1.23.0

================
File: results/cv1_result.json
================
{
  "file_name": "cv1.pdf",
  "processed_at": "2025-02-12T21:48:22.803565",
  "cv_data": {
    "name": "LINH GIA NGUYEN",
    "location": "HCMC, Vietnam",
    "social": [],
    "email": "nglinh@aseanyouth.net",
    "linkedin": null,
    "phone": "+84983182346",
    "intro": "Female | DOB: 12 Jul 2007 | Nationality: Vietnamese",
    "education": [
      {
        "school": "Nguyen Du Secondary School",
        "class_year": "2018 - 2022",
        "major": "",
        "minor": null,
        "gpa": 9.8
      },
      {
        "school": "Tran Dai Nghia High School for the Gifted",
        "class_year": "Expected May 2025",
        "major": "",
        "minor": null,
        "gpa": null
      }
    ],
    "professional_experience": [
      {
        "company": "University of South Florida - Laboratory for Advanced Materials and Sensor Technologies",
        "location": "Remote",
        "position": "Research Assistant",
        "seniority": "",
        "duration": "2022 - Present",
        "description": "Supervised by Professor Manh-Huong Phan. Research Intern in various projects. Co-author & Research Assistant in international Research Paper. Lead Research Assistant in AI-DURST project."
      },
      {
        "company": "New York Academy of Sciences Projects",
        "location": "Remote",
        "position": "Team Leader & Researcher",
        "seniority": "",
        "duration": "2023 - Present",
        "description": "Led a team to develop technology-driven applications for education and health outcomes."
      },
      {
        "company": "University Medical Center HCMC",
        "location": "HCMC",
        "position": "Research Intern",
        "seniority": "",
        "duration": "2023 - Present",
        "description": "Project: AI-Powered Diagnostic Ultra-Sensitive Respiratory Magnetic Sensor Technology. Supervised by Asso. Professor Nguyen Hoang Dinh."
      },
      {
        "company": "FPT Software Company",
        "location": "HCMC",
        "position": "Python Web Developer Intern",
        "seniority": "",
        "duration": "July 2024 - September 2024",
        "description": "Project: AquaEmi - A System for Monitoring and Managing Water Quality Using Geolocation and Artificial Intelligence."
      }
    ],
    "projects": [],
    "awards": [
      {
        "contest": "April Olympic Festival 2023-2024",
        "prize": "Gold Medal in Literature",
        "description": "",
        "role": null,
        "link": null,
        "time": ""
      },
      {
        "contest": "Code Like a Girl Competition 2023",
        "prize": "Top 14 Teams in the Final Round",
        "description": "",
        "role": null,
        "link": null,
        "time": ""
      },
      {
        "contest": "CiC Creative Entrepreneurship Ideas Competition 2023",
        "prize": "Top 15 Teams in the Final Round",
        "description": "",
        "role": null,
        "link": null,
        "time": ""
      },
      {
        "contest": "G-College Hackathon Competition 2024",
        "prize": "Top 6 Teams in the Final Round",
        "description": "",
        "role": null,
        "link": null,
        "time": ""
      },
      {
        "contest": "Techcombank HCMC International Marathon & VnExpress Marathon HCMC Midnight",
        "prize": "Completed 10km and 21km distances",
        "description": "",
        "role": null,
        "link": null,
        "time": ""
      }
    ],
    "certifications": [],
    "skills": [
      {
        "name": "Programming Language",
        "list": [
          "Python",
          "MySQL",
          "HTML/CSS",
          "JavaScript",
          "MATLAB",
          "Java"
        ]
      },
      {
        "name": "Languages",
        "list": [
          "Vietnamese (Native)",
          "English (Fluent)",
          "Chinese (Intermediate)",
          "French (Beginner)"
        ]
      },
      {
        "name": "Sports",
        "list": [
          "Basketball",
          "Swimming",
          "Marathon"
        ]
      },
      {
        "name": "Skills",
        "list": [
          "Writing Fiction",
          "Scientific Research",
          "Playing Guitar",
          "Cooking"
        ]
      }
    ]
  },
  "scores": {
    "education": 630.0,
    "experience": 1025.0,
    "projects": 0.0,
    "awards": 5225.0,
    "certifications": 0.0,
    "total_score": 1134.5
  },
  "status": "Pass",
  "ai_summary": "Linh Gia Nguyen, based in HCMC, Vietnam, is a high-achieving student with a strong background in research and technology. She attended Nguyen Du Secondary School, achieving a GPA of 9.8, and is currently studying at Tran Dai Nghia High School for the Gifted, expected to graduate in May 2025.\n\nProfessionally, Linh is a Research Assistant at the University of South Florida's Laboratory for Advanced Materials and Sensor Technologies, contributing to various projects and co-authoring a research paper. She also leads a team at the New York Academy of Sciences Projects, focusing on technology-driven educational and health applications. Additionally, she is a Research Intern at University Medical Center HCMC working on AI-powered diagnostic technology and completed an internship as a Python Web Developer at FPT Software Company.\n\nLinh has received several awards, including a Gold Medal in Literature at the April Olympic Festival and recognition in multiple competitions, such as the Code Like a Girl Competition and CiC Creative Entrepreneurship Ideas Competition. Her skills include programming languages like Python and MATLAB, fluency in several languages, and interests in sports and creative activities.",
  "ai_evaluation": "The resume of Linh Gia Nguyen presents an impressive array of accomplishments and experiences, especially given her young age. Here are some detailed evaluations and suggestions:\n\n### Strengths:\n\n1. **Education:**\n   - Linh maintains an excellent academic record, as reflected by a GPA of 9.8 at Nguyen Du Secondary School. Her enrollment in Tran Dai Nghia High School for the Gifted, with an expected graduation in 2025, highlights a strong academic background.\n\n2. **Professional Experience:**\n   - Despite her young age, Linh has gained significant research experience, particularly in advanced and technical fields. Her role as a Research Assistant at the University of South Florida and as a Team Leader & Researcher for the New York Academy of Sciences Projects stands out.\n   - Her ability to lead projects and co-author research papers suggests strong leadership and collaborative skills.\n\n3. **Awards:**\n   - Linh has achieved recognition in various competitions, notably winning a Gold Medal in Literature at the April Olympic Festival. Her participation in hackathons and entrepreneurship competitions also demonstrates her versatility and willingness to engage in diverse challenges.\n\n4. **Skills:**\n   - The programming languages and technical skills listed, including Python, MySQL, MATLAB, and Java, are highly valuable and suggest strong technical proficiency.\n   - Multilingual capabilities in Vietnamese, English, Chinese, and French add to her versatility and potential for international collaboration.\n\n5. **Extracurricular Activities:**\n   - Her involvement in sports and music (playing guitar) indicates a well-rounded personality with interests beyond academics and research.\n\n### Areas for Improvement:\n\n1. **Resume Structure:**\n   - Consider organizing the resume into clear sections with headers such as \"Education,\" \"Professional Experience,\" \"Projects,\" \"Awards,\" \"Skills,\" and \"Certifications\" (even if empty for now). This will improve readability and structure.\n\n2. **Professional Experience Details:**\n   - Providing specific dates for roles (month/year) can offer more clarity on the duration of each experience.\n   - For each role, include specific achievements or contributions. For example, detail the outcomes of the AI-DURST project or any specific technologies used in the Python Web Developer Intern role.\n\n3. **Projects:**\n   - Although no projects are listed, Linh could consider detailing any significant school, personal, or professional projects. This would provide additional context to her skills and experiences.\n\n4. **Certifications:**\n   - If Linh has any certifications, especially in programming or languages, listing them would add additional credibility to her skills.\n\n5. **Contact Information:**\n   - Including a LinkedIn profile, if available, would enhance professional networking opportunities. If there are any social media profiles relevant to her professional life, consider including them.\n\n6. **Formatting and Aesthetics:**\n   - Ensure the resume is visually appealing with consistent font sizes, styles, and spacing. Use bullet points for lists to enhance clarity.\n\n### Conclusion:\n\nLinh Gia Nguyen's resume reflects a strong academic and professional trajectory, highlighting her potential and diverse skill set. By refining the organization and adding more detailed descriptions, her resume can be even more compelling to future educational institutions or employers."
}

================
File: results/summary_report.json
================
{
  "processed_at": "2025-02-12T21:48:22.818109",
  "total_cvs": 1,
  "successful": 1,
  "failed": 0,
  "passed": 1,
  "failed_screening": 0,
  "average_scores": {
    "education": 630.0,
    "experience": 1025.0,
    "projects": 0.0,
    "awards": 5225.0,
    "certifications": 0.0,
    "total_score": 1134.5
  },
  "results": [
    {
      "file_name": "cv1.pdf",
      "processed_at": "2025-02-12T21:48:22.803565",
      "cv_data": {
        "name": "LINH GIA NGUYEN",
        "location": "HCMC, Vietnam",
        "social": [],
        "email": "nglinh@aseanyouth.net",
        "linkedin": null,
        "phone": "+84983182346",
        "intro": "Female | DOB: 12 Jul 2007 | Nationality: Vietnamese",
        "education": [
          {
            "school": "Nguyen Du Secondary School",
            "class_year": "2018 - 2022",
            "major": "",
            "minor": null,
            "gpa": 9.8
          },
          {
            "school": "Tran Dai Nghia High School for the Gifted",
            "class_year": "Expected May 2025",
            "major": "",
            "minor": null,
            "gpa": null
          }
        ],
        "professional_experience": [
          {
            "company": "University of South Florida - Laboratory for Advanced Materials and Sensor Technologies",
            "location": "Remote",
            "position": "Research Assistant",
            "seniority": "",
            "duration": "2022 - Present",
            "description": "Supervised by Professor Manh-Huong Phan. Research Intern in various projects. Co-author & Research Assistant in international Research Paper. Lead Research Assistant in AI-DURST project."
          },
          {
            "company": "New York Academy of Sciences Projects",
            "location": "Remote",
            "position": "Team Leader & Researcher",
            "seniority": "",
            "duration": "2023 - Present",
            "description": "Led a team to develop technology-driven applications for education and health outcomes."
          },
          {
            "company": "University Medical Center HCMC",
            "location": "HCMC",
            "position": "Research Intern",
            "seniority": "",
            "duration": "2023 - Present",
            "description": "Project: AI-Powered Diagnostic Ultra-Sensitive Respiratory Magnetic Sensor Technology. Supervised by Asso. Professor Nguyen Hoang Dinh."
          },
          {
            "company": "FPT Software Company",
            "location": "HCMC",
            "position": "Python Web Developer Intern",
            "seniority": "",
            "duration": "July 2024 - September 2024",
            "description": "Project: AquaEmi - A System for Monitoring and Managing Water Quality Using Geolocation and Artificial Intelligence."
          }
        ],
        "projects": [],
        "awards": [
          {
            "contest": "April Olympic Festival 2023-2024",
            "prize": "Gold Medal in Literature",
            "description": "",
            "role": null,
            "link": null,
            "time": ""
          },
          {
            "contest": "Code Like a Girl Competition 2023",
            "prize": "Top 14 Teams in the Final Round",
            "description": "",
            "role": null,
            "link": null,
            "time": ""
          },
          {
            "contest": "CiC Creative Entrepreneurship Ideas Competition 2023",
            "prize": "Top 15 Teams in the Final Round",
            "description": "",
            "role": null,
            "link": null,
            "time": ""
          },
          {
            "contest": "G-College Hackathon Competition 2024",
            "prize": "Top 6 Teams in the Final Round",
            "description": "",
            "role": null,
            "link": null,
            "time": ""
          },
          {
            "contest": "Techcombank HCMC International Marathon & VnExpress Marathon HCMC Midnight",
            "prize": "Completed 10km and 21km distances",
            "description": "",
            "role": null,
            "link": null,
            "time": ""
          }
        ],
        "certifications": [],
        "skills": [
          {
            "name": "Programming Language",
            "list": [
              "Python",
              "MySQL",
              "HTML/CSS",
              "JavaScript",
              "MATLAB",
              "Java"
            ]
          },
          {
            "name": "Languages",
            "list": [
              "Vietnamese (Native)",
              "English (Fluent)",
              "Chinese (Intermediate)",
              "French (Beginner)"
            ]
          },
          {
            "name": "Sports",
            "list": [
              "Basketball",
              "Swimming",
              "Marathon"
            ]
          },
          {
            "name": "Skills",
            "list": [
              "Writing Fiction",
              "Scientific Research",
              "Playing Guitar",
              "Cooking"
            ]
          }
        ]
      },
      "scores": {
        "education": 630.0,
        "experience": 1025.0,
        "projects": 0.0,
        "awards": 5225.0,
        "certifications": 0.0,
        "total_score": 1134.5
      },
      "status": "Pass",
      "ai_summary": "Linh Gia Nguyen, based in HCMC, Vietnam, is a high-achieving student with a strong background in research and technology. She attended Nguyen Du Secondary School, achieving a GPA of 9.8, and is currently studying at Tran Dai Nghia High School for the Gifted, expected to graduate in May 2025.\n\nProfessionally, Linh is a Research Assistant at the University of South Florida's Laboratory for Advanced Materials and Sensor Technologies, contributing to various projects and co-authoring a research paper. She also leads a team at the New York Academy of Sciences Projects, focusing on technology-driven educational and health applications. Additionally, she is a Research Intern at University Medical Center HCMC working on AI-powered diagnostic technology and completed an internship as a Python Web Developer at FPT Software Company.\n\nLinh has received several awards, including a Gold Medal in Literature at the April Olympic Festival and recognition in multiple competitions, such as the Code Like a Girl Competition and CiC Creative Entrepreneurship Ideas Competition. Her skills include programming languages like Python and MATLAB, fluency in several languages, and interests in sports and creative activities.",
      "ai_evaluation": "The resume of Linh Gia Nguyen presents an impressive array of accomplishments and experiences, especially given her young age. Here are some detailed evaluations and suggestions:\n\n### Strengths:\n\n1. **Education:**\n   - Linh maintains an excellent academic record, as reflected by a GPA of 9.8 at Nguyen Du Secondary School. Her enrollment in Tran Dai Nghia High School for the Gifted, with an expected graduation in 2025, highlights a strong academic background.\n\n2. **Professional Experience:**\n   - Despite her young age, Linh has gained significant research experience, particularly in advanced and technical fields. Her role as a Research Assistant at the University of South Florida and as a Team Leader & Researcher for the New York Academy of Sciences Projects stands out.\n   - Her ability to lead projects and co-author research papers suggests strong leadership and collaborative skills.\n\n3. **Awards:**\n   - Linh has achieved recognition in various competitions, notably winning a Gold Medal in Literature at the April Olympic Festival. Her participation in hackathons and entrepreneurship competitions also demonstrates her versatility and willingness to engage in diverse challenges.\n\n4. **Skills:**\n   - The programming languages and technical skills listed, including Python, MySQL, MATLAB, and Java, are highly valuable and suggest strong technical proficiency.\n   - Multilingual capabilities in Vietnamese, English, Chinese, and French add to her versatility and potential for international collaboration.\n\n5. **Extracurricular Activities:**\n   - Her involvement in sports and music (playing guitar) indicates a well-rounded personality with interests beyond academics and research.\n\n### Areas for Improvement:\n\n1. **Resume Structure:**\n   - Consider organizing the resume into clear sections with headers such as \"Education,\" \"Professional Experience,\" \"Projects,\" \"Awards,\" \"Skills,\" and \"Certifications\" (even if empty for now). This will improve readability and structure.\n\n2. **Professional Experience Details:**\n   - Providing specific dates for roles (month/year) can offer more clarity on the duration of each experience.\n   - For each role, include specific achievements or contributions. For example, detail the outcomes of the AI-DURST project or any specific technologies used in the Python Web Developer Intern role.\n\n3. **Projects:**\n   - Although no projects are listed, Linh could consider detailing any significant school, personal, or professional projects. This would provide additional context to her skills and experiences.\n\n4. **Certifications:**\n   - If Linh has any certifications, especially in programming or languages, listing them would add additional credibility to her skills.\n\n5. **Contact Information:**\n   - Including a LinkedIn profile, if available, would enhance professional networking opportunities. If there are any social media profiles relevant to her professional life, consider including them.\n\n6. **Formatting and Aesthetics:**\n   - Ensure the resume is visually appealing with consistent font sizes, styles, and spacing. Use bullet points for lists to enhance clarity.\n\n### Conclusion:\n\nLinh Gia Nguyen's resume reflects a strong academic and professional trajectory, highlighting her potential and diverse skill set. By refining the organization and adding more detailed descriptions, her resume can be even more compelling to future educational institutions or employers."
    }
  ]
}

================
File: test.py
================
import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, List
from datetime import datetime

# Update imports to use absolute imports
from app.modules.document_extraction.extractor import extract_resume
from app.modules.scoring.scorer import calculate_total_score
from app.modules.summarization.summarizer import summarize_resume
from app.modules.summarization.evaluator import evaluate_resume
from app.modules.scoring.education import calculate_education_score
from app.modules.scoring.experience import calculate_experience_score
from app.modules.scoring.projects import calculate_projects_score
from app.modules.scoring.awards import calculate_awards_score
from app.modules.scoring.certifications import calculate_certifications_score

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cv_processing.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def process_cv(file_path: str) -> Dict[str, Any]:
    """
    Process a single CV file and return evaluation results.
    
    Args:
        file_path (str): Path to the CV file
        
    Returns:
        Dict[str, Any]: Dictionary containing evaluation results
    """
    try:
        logger.info(f"Processing CV: {os.path.basename(file_path)}")
        
        # Extract structured data from CV
        resume = extract_resume(file_path)
        
        # Calculate scores
        scoring_result = calculate_total_score(resume)
        detailed_scores = {
            "education": calculate_education_score(resume.education),
            "experience": calculate_experience_score(resume.professional_experience),
            "projects": calculate_projects_score(resume.projects),
            "awards": calculate_awards_score(resume.awards),
            "certifications": calculate_certifications_score(resume.certifications),
            "total_score": scoring_result["total_score"]
        }
        
        # Generate summary and evaluation
        summary = summarize_resume(resume)
        evaluation = evaluate_resume(resume)
        
        # Prepare results
        results = {
            "file_name": os.path.basename(file_path),
            "processed_at": datetime.now().isoformat(),
            "cv_data": resume.dict(),
            "scores": detailed_scores,
            "status": "Pass" if scoring_result["total_score"] >= 70 else "Fail",
            "ai_summary": summary,
            "ai_evaluation": evaluation
        }
        
        logger.info(f"Successfully processed {os.path.basename(file_path)}")
        return results
        
    except Exception as e:
        logger.error(f"Error processing {os.path.basename(file_path)}: {str(e)}")
        return {
            "file_name": os.path.basename(file_path),
            "processed_at": datetime.now().isoformat(),
            "error": str(e),
            "status": "Error"
        }

def process_directory(directory_path: str, output_dir: str) -> None:
    """
    Process all CV files in a directory and save results.
    
    Args:
        directory_path (str): Path to directory containing CVs
        output_dir (str): Path to directory for saving results
    """
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # Get all PDF files
    cv_files = list(Path(directory_path).glob("*.pdf"))
    
    if not cv_files:
        logger.warning(f"No PDF files found in {directory_path}")
        return
    
    logger.info(f"Found {len(cv_files)} PDF files to process")
    
    # Process each CV
    all_results = []
    for cv_file in cv_files:
        results = process_cv(str(cv_file))
        all_results.append(results)
        
        # Save individual result
        result_file = Path(output_dir) / f"{cv_file.stem}_result.json"
        save_json(results, str(result_file))
    
    # Save summary report
    summary_report = create_summary_report(all_results)
    save_json(summary_report, str(Path(output_dir) / "summary_report.json"))
    
def create_summary_report(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Create a summary report from all processed CVs.
    
    Args:
        results (List[Dict[str, Any]]): List of individual CV results
        
    Returns:
        Dict[str, Any]: Summary report
    """
    total_cvs = len(results)
    successful = len([r for r in results if r.get("status") != "Error"])
    passed = len([r for r in results if r.get("status") == "Pass"])
    
    # Calculate average scores for successful processes
    avg_scores = {
        "education": 0.0,
        "experience": 0.0,
        "projects": 0.0,
        "awards": 0.0,
        "certifications": 0.0,
        "total_score": 0.0
    }
    
    for result in results:
        if "scores" in result:
            for category, score in result["scores"].items():
                avg_scores[category] += score
    
    if successful > 0:
        avg_scores = {k: v/successful for k, v in avg_scores.items()}
    
    return {
        "processed_at": datetime.now().isoformat(),
        "total_cvs": total_cvs,
        "successful": successful,
        "failed": total_cvs - successful,
        "passed": passed,
        "failed_screening": successful - passed,
        "average_scores": avg_scores,
        "results": results
    }

def save_json(data: Dict[str, Any], file_path: str) -> None:
    """
    Save data as JSON file.
    
    Args:
        data (Dict[str, Any]): Data to save
        file_path (str): Output file path
    """
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        logger.info(f"Saved results to {file_path}")
    except Exception as e:
        logger.error(f"Error saving results to {file_path}: {str(e)}")

def main():
    """
    Main function to run the CV processing system.
    """
    # Configuration
    INPUT_DIR = "cvs"  # Directory containing CVs
    OUTPUT_DIR = "results"  # Directory for results
    
    try:
        logger.info("Starting CV processing system")
        
        # Create directories if they don't exist
        Path(INPUT_DIR).mkdir(exist_ok=True)
        Path(OUTPUT_DIR).mkdir(exist_ok=True)
        
        # Process all CVs in the input directory
        process_directory(INPUT_DIR, OUTPUT_DIR)
        
        logger.info("CV processing completed")
        
    except Exception as e:
        logger.error(f"Error in main process: {str(e)}")

if __name__ == "__main__":
    main()
